you note that your estimate of the burden of writing
canonizers by hand may be inaccurate, as you're actually working
with generated code rather than genuine hand-written code.  Can
you give any estimate of how faithfully the generated code
corresponds to code that might be written by hand?  e.g. whether
the human-written version actually would be smaller, and by how
much?

  For BS, we think the generated code closely corresponds to code that might be
  written by hand.  We put in effort to only count common subexpressions once
  (for example, if we define a regular expression, and that regular expression
  is used in the generated canonizer - we only count that as 1 AST node, instead
  of a count corresponding to the size of the regular expression).

  For NS, we think the generated code less closely corresponds to code that
  might be written by hand.  While the original Optician system puts in effort
  to minimize the size of the ASTs, it is imperfect, and does not perform tasks
  like common subexpression elimination.  Furthermore, quotient lenses allow for
  canonizers to be interspersed with other lens combinators.  When we move these
  these canonizers to the edges, additional scaffolding code is required, and
  included in our counts.  We can give additional information detailing this
  difference.


Fig. 10: What are CS and LS?  Are they supposed to be BS and NS?

  Yes, we submitted a version with incorrect graphs - those should be BS and NS.

927: I am a bit concerned about "on average" here: is this a
geometric mean?

  No, we used an arithmetic mean for all these averages.  Using  geometric means
  the sentences would be as follows:

  On average, BS used 38.5% more AST nodes than QS, requiring an average of 214
  more AST nodes. On average, NS used 180% more AST nodes than QS, requiring an
  average of 998 more AST nodes.


Fig. 11: OO, PO, PP are not defined, although I can guess what
they correspond to from the caption.  But what are these 40-50
benchmarks?  and why do the caption and text say "able to
synthesize all quotient lenses in under 10 seconds", while the
graph (b) appears to be still increasing at 15s?

  Sorry, we submitted a version with incorrect graphs.  We made additional
  benchmarks corresponding to some text transformations from data.gov.
  Eventually, we found the paper read better without having to explain these
  additional benchmarks, and so removed them from our writeup.  The benchmark
  that took 15 seconds was due to slowness in Optician -- when we made it
  bijective, vanilla Optician took a similar amount of time.
