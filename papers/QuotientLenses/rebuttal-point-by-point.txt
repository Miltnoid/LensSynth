This is a well-written paper with a clearly defined result that seems
like a small but useful increment over previous work.

    In our view, our paper presents two new innovations which can be applied in a broad
    variety of situations.  First, our quotient regular expressions (QREs) are the 
    first innovation that we know of that can be used to define a broad class of 
    equivalence relations on regular languages by simply adding annotations to 
    ordinary regular expressions. Secondly, in our work, we show how to adapt an 
    existing synthesis algorithm to solve a more complicated synthesis task; we 
    believe that this method is likely to be applicable in other synthesis tasks

The main result is a kind of compositionality theorem: you can compose
QRE lenses freely, and the result is guaranteed to be normalizable.
But I wonder about the lack of compositionality in the construction of
QREs, since the Â· and | combinators cannot be used to combine
arbitrary expressions due to the strong unambiguity constraint
(Section 4.4).  Does your experience say anything about whether this
constraint is a problem in practice?

    Indeed unambiguity constraints are an issue in practice. On the other hand
    these constraints are common with programs that operate in a similar fashion
    to regular transducers; indeed several existing systems such as Boomerang and 
    DRex impose similar constraints. QRE lens synthesis however lessens the
    burden on the programmer since she only needs to deal with these constraints
    when writing QREs; without synthesis then the programmer would have to consider
    similar constraints when specifying the program,
    when writing the canonizer, and when writing the lens.

you note that your estimate of the burden of writing
canonizers by hand may be inaccurate, as you're actually working
with generated code rather than genuine hand-written code.  Can
you give any estimate of how faithfully the generated code
corresponds to code that might be written by hand?  e.g. whether
the human-written version actually would be smaller, and by how
much?

  For BS, we think the generated code closely corresponds to code that might be
  written by hand.  We put in effort to only count common subexpressions once
  (for example, if we define a regular expression, and that regular expression
  is used in the generated canonizer - we only count that as 1 AST node, instead
  of a count corresponding to the size of the regular expression).

  For NS, we think the generated code less closely corresponds to code that
  might be written by hand.  While the original Optician system puts in effort
  to minimize the size of the ASTs, it is imperfect, and does not perform tasks
  like common subexpression elimination.  Furthermore, quotient lenses allow for
  canonizers to be interspersed with other lens combinators.  When we move these
  these canonizers to the edges, additional scaffolding code is required, and
  included in our counts.  We can give additional information detailing this
  difference.


Fig. 10: What are CS and LS?  Are they supposed to be BS and NS?

  Yes, we submitted a version with incorrect graphs - those should be BS and NS.

927: I am a bit concerned about "on average" here: is this a
geometric mean?

  No, we used an arithmetic mean for all these averages.  Using  geometric means
  the sentences would be as follows:

  On average, BS used 38.5% more AST nodes than QS, requiring an average of 214
  more AST nodes. On average, NS used 180% more AST nodes than QS, requiring an
  average of 998 more AST nodes.


Fig. 11: OO, PO, PP are not defined, although I can guess what
they correspond to from the caption.  But what are these 40-50
benchmarks?  and why do the caption and text say "able to
synthesize all quotient lenses in under 10 seconds", while the
graph (b) appears to be still increasing at 15s?

  Sorry, we submitted a version with incorrect graphs.  We made additional
  benchmarks corresponding to some text transformations from data.gov.
  Eventually, we found the paper read better without having to explain these
  additional benchmarks, and so removed them from our writeup.  The benchmark
  that took 15 seconds was due to slowness in Optician -- when we made it
  bijective, vanilla Optician took a similar amount of time.
