Thank you for the thoughtful feedback. We first respond briefly to major
questions and concerns, then provide an (optional, "below the fold")
point-by-point response to individual concerns. We are happy to include any
of the information presented in this rebuttal in the final version of the
paper.



    - Why is writing two regexes easier for the user than writing a lens? Do
      input formats often come equipped with regexes already? Can the
      synthesizer use such pre-existing regexes out of the box, or do they have
      to be "massaged" by the user?

Writing regular expressions is significantly easier than programming lenses:

* Regular expressions are widely understood, while few programmers know
  about programming with lenses.

* Each of the two regular expressions describes just one data format, while
  a lens has to deal with both at the same time.

* In the (common, in our experience) case where the natural way of writing
  the two formats puts Kleene stars in different places, manual lens
  programming requires mentally transforming both to a common "aligned"
  form, which can be quite challenging for programmers.  Our synthesis
  algorithm does this alignment automatically.

In our experiments, we manually programmed the regular expressions. While
writing these regular expressions, we did not massage them to be amenable to
synthesis. The synthesizer implements the star-semiring regular
expression equivalence axioms so that users do not have to worry about
the way they write expressions themselves.



    - What is the relationship between our information-theoretic ranking, and
      prior work like Unsupervised Learning by Program Synthesis

In a nutshell, Unsupervised Learning by Program Synthesis (ULPS)
learns a description of a single data set.  Ideally, such 
descriptions are compact and information theory is used as a measure
of the compactness of the description learned.  In this sense, ULPS
shares more similarities with Fisher et al's paper on "From Dirt To
Shovels" (which also seeks to learn a description of a single data
source) than it does with the current paper.

In contrast, in our current paper, we are attempting to learn a
translation from data set to another.  However, there are many different
candidate translations.  To select amongst the candidate translations,
we choose the translation that preserves as much information from
source to target (and vice versa) as possible.

In summary, both systems use information theory but the objectives of
the two systems are different and the properties measured by
information theory are also different.



    - The authors do not consider lens compositions in their synthesis. Are
      there any justifications not to consider them? In existing results that
      synthesize bijective (quotient) lenses, one of the reasons not to consider
      lens composition in their synthesis is that ruling out them does not
      change the expressive power? Can you say a similar argument for this case?

We do not synthesize lens compositions for the same reasons as
previous work -- such compositions are quite difficult to generate
(doing so requires synthesizing an intermediate regular expression
from out of thin air).  We conjecture that adding composition lenses
would not increase the expressive power of the language but we have
not proven this fact.  The analogous proof for bijective lenses
required nearly 100 pages of work and we did not believe the result
was important enough to invest this amount of work.  We chose to focus
our energy on understanding the information theoretic ranking methods,
and the relationship between simple symmetric lenses and classical
symmetric lenses, which we view as the core contributions of this paper.



    - There is no explanation of the selection of the benchmarks. What are their
      characteristics and why those exercise interesting aspects of the
      algorithm.
      
We describe the selection of the benchmark suite in section 6.1.

29 of the benchmarks are taken from Augeas, a bidirectional tool for
editing Linux configuration files.  We selected these benchmarks by
selecting Augeas files one-by-one in alphabetical order (we did
not cherry-pick programs).  We stopped when we didn't feel that
adding more benchmarks would be informative. These examples were
quite large, and involved some very complex transformations, though
these benchmarks were bijective. These benchmarks demonstrated 
how our tool performed on large, complex transformations.

The 8 benchmarks from Flash Fill provided a range of transformations,
none of which were bijective. Again, the FlashFill benchmarks were
the first 8 benchmarks in the paper.  These benchmarks demonstrated
how our tool performed on transformation tasks that discard large
amounts of data.  On occasion, these benchmarks require the use of
many regular expression equivalence axioms.

The remaining 11 benchmarks were chosen in an ad hoc manner --- we
simply looked around for day-to-day data formats for which it would
be useful to have lenses.



    - Why is symmetric synthesis slower than bijective synthesis.

Both the bijective synthesis and the symmetric synthesis engines use a
pair of collaborating synthesizers that involves (1) a search for a
compatible pair of regular expressions and (2) search for a lens given
those regular expressions.

Bijective synthesis is faster than symmetric synthesis because part
(2) is much faster.  More specifically, a bijection must translate
*all* data on the left into data on the right.  However, a symmetric
synthesis problem has a choice of *which* data on the left to translate
into data on the right.  This gives rise to a large set of possible
choices, and symmetric synthesis must consider all of them.


=================================================================================
Reviewer A
=================================================================================

    - Why is writing two regexes easier for the user than writing a lens? Do
      input formats often come equipped with regexes already? Can the
      synthesizer use such pre-existing regexes out of the box, or do they have
      to be "massaged" by the user? (Extended Response)

Below is an explicit example of of the difficulty of writing lenses vs writing
regular expressions.

        let last_head = NAME . "," . (WSP . NAME)*
        let last_tail = (NAME . WSP)* NAME

        let name_conversion : last_head <=> last_tail =
            lens_swap
                (NAME . del ",")
                (lens_swap WSP NAME)*

While the name_conversion expression has about as many AST nodes than the two
regular expressions, we find it much harder to write.

Engineering best practices dictate that we annotate the lenses with their
types.  While Boomerang does not require these annotations - being able to
infer the types from the terms - the types of the term serve as
documentation for future programmers to understand what formats the lens
maps between.  Furthermore, the types provide resilience in the face of future
lens modifications, as the types ensure that the lens maps exactly
between strings of the provided formats. Invalid inputs are not accepted
nor are the outputs ever invalid.  Consequently, we are not requiring
additional work. Instead, we are saving programmers from having to 
writing the lens by leveraging work they should already be performing.



    - Moreover, on top of the regexes, the user needs to provide examples and
      relevance annotations, and the only way to do so seems to be through an
      iterative process (by inspecting the synthesized lens and adding more
      examples / annotations when needed). Such iterative approach would be
      challenging for a user who is not sufficiently familiar with lenses to
      write one themselves.

We described the iterative process assuming a lazy user who didn't want to
provide more specifications than were necessary. However, such an iterative
process is not required, and if the user is confident that certain information
is "required" or may safely "skipped," they can safely add this information --
it will not slow down synthesis. Similarly, if the user already has a benchmark
suite, they may safely include such examples.



    - To add to the previous point: the evaluation could do a better job
      quantifying these limitations. For example, it would be good to see for
      each benchmark the size of annotations required, as well as how many
      "existing lenses" were used in compositional synthesis and where they came
      from (some standard library of lenses? or did the authors manually
      decompose each problem into subproblems? if so, then how many subproblems
      were there?)

There were a total of 12 uses of "require" in the benchmark suite. There were a
total of 4 uses of "skip" in the benchmark suite. In total 73 existing lenses
were used in compositional synthesis (about 1.5 per benchmark on average).
Typically, we manually decomposed these lenses, though some used the to_upper
function. We are happy to include a figure detailing the subtasks specified
during compositional synthesis in the final version of the paper (similar to the
one on p24 of Synthesizing Bijective Lenses).



    - Finally, the evaluation could also do a better job confirming that there's
      indeed a benefit to synthesizing a lens, rather than using a tool like
      FlashFill to synthesize the two functions independently. The related work
      section mentions that FlashFill cannot handle nested loops. However,
      FlashFill has evolved a lot since its original publication in 2011, so it
      would be interesting to see an empirical comparison.

We included an extended comparison to a current version of FlashFill in previous
work, and did not include such a comparison in this work for lack of space. We
are happy to make this comparison to FlashFill cited more explicitly.



    - The introduction mentions that stochastic regular expressions guide the
      search process. I would like to see it better explained in the main text
      of the paper: do they actually guide the search or merely rank the
      results?

Due to the Greedy nature of GreedySynth, the stochastic regular expressions
guide the search (insofar as the lens costs guide the search).



    - L 284: please explain why this rule is needed for the lens in Fig. 3 to be
      well-typed.

Without the type equivalence rule, the lens would be well-typed between
emp_salaries and header . ("" | ("\n" . emp_ins) . ("\n" . emp_ins)∗). To be
well-typed between emp_salaries and emp_insurance, the right-hand type needs to
be transformed into an equivalent form, without the explicit star unrolling.



    - Section 5.6: expansion inference seems to be a crucial part of the
      algorithm but is described very briefly. I would like to see more detail.

Expansion inference is the same as is performed in bijective lens inference, and
we did not include it for space. We will gladly make such a citation more
explicit.



    - Can you explain why you need the lower bound [for intervals]?

While we do not need it in the algorithm, we include it to make formal claims
about bounding the cost. We describe why we need intervals for the formal claims
in 4.1.



=================================================================================
Reviewer B
=================================================================================

    - Theorem 1 is about the soundness of the equational theory shown in Fig. 4.
      Can you say the completeness?

The equational theory shown in Fig. 4. is incomplete, for it does not include
some of the non-star-semiring equivalences (for example: the regular expression
equivalence of (A | B)* = (A*B)*A* does not currently have an analogue in this
equational theory). Furthermore, it is currently unclear whether the natural
extensions to Conway's equational theory is sufficient for completeness, and
whether Kozen and Solomaa's deductive systems are easily amenable to SREs.



    - p10:L466 for other reasons

The other reasons are to guarantee that any lens that is synthesized by our
synthesis algorithm is guaranteed to be well-typed.



    - p10:L489 relaxing this restrict is not problemantic

For all these combinators, we can follow the style of
unambiguity requirements on the types that was used in prior lens work. For
example, the original Boomerang paper explicitly includes such unambiguity
checks within the typing judgments; these unambiguity checks can be used in the
same way for the symmetric lens variants of these combinators.



    - p12:L563-572 I would like to read more explanations on why
      \mathbb{H}^{\to} is defined as this.

In general, these derivations follow the information theory, so as to make
Theorem 3 true. Most strictly follow the rules of information theory.
The only ones that are more complex are merges. Calculating the expected
information to recover a string from a lens using merges does not involve a
simple inductive calculation, but requires reasoning about the probabilities of
individual strings in the unioned format, conditioned on the probabilities of
the source format. Because of this complexity, we chose to bound the information
content instead of calculate it exactly.



    - p13:L559 Cost(\ell) < Cost(best). What is the meaning of "<"? Since
      Cost(\ell) can return a range instead of a number, the definition of "<"
      is not so obvious.

We define the cost as the sum of the maximum of \mathbb{H}^{\to} and the maximum
of \mathbb{H}^{\leftarrow}.



    - p14:L650 distance d.  What is "distance"?

The distance is the number of star-semiring equivalences that must be applied to
the pair of regular expressions to convert to the pair.



    - p17:L806 CannotMap determines whether there is no lens satisfying the
      examples.

This is done by looking at the example parses. For example, given two regular
expressions "a" | "b" and "c", and the example data putl "c" "b" = "a", we can
determine that there cannot be a lens between "c" and "b".



=================================================================================
Reviewer C
=================================================================================

    - 5 runs per benchmark are not sufficient to draw statistically significant
      conclusions. There is no report of errors or confidence intervals for the
      mean time calculations.

We only ran 5 times because there was so little variance between the times, as
our procedure is deterministic. Rerunning, we found our average standard
deviation while running the full symmetric algorithm was .0162. We are happy
to both include standard deviations and rerun additional times.



    - The graphs drop a lot of useful information. In particular they do not
      show for which benchmark SS does better than SSNC or BS. This information
      together with an analysis of the benchmarks can help understand the
      performance of the proposed algorithm. The number of benchmarks that
      terminate in less than 30s offers almost no insight.

In general, SS always performs better than SSNC and SS always performs worse
than BS. We ran for 30s under the assumption that users would be unwilling to
wait for responses much longer than that. We are happy to rerun with a longer
timeout.



    - Similar to the above, the graph in figure 8 and the accompanying
      discussion are not sufficient to understand what features of the algorithm
      help or do not help with what kind of inputs.

Requires help with continuing searching when the desired lenses require a large
number of Expand applications. Skips help when the desired lenses require
projecting large amounts of information. Using the DC metric only works for
lenses where the desired lens appears quite early. FL is the same as DC, but is
even more dependent on the desired lens appearing early (it *must* appear in the
first cluster containing a satisfying program).
