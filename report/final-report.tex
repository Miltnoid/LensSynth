\documentclass[12pt]{article}

\newif\ifdraft\drafttrue  % set true to show comments
% \newif\ifdraft\draftfalse  % set true to show comments
\newif\ifanon\anonfalse    % set true to suppress names, etc.
\newif\ifappendices\appendicesfalse

%\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage[capitalise]{cleveref}
\usepackage{makecell}%To keep spacing of text in tables
\usepackage{nccmath}
\usepackage{mathtools}
\usepackage{bussproofs}
\usepackage{varwidth}
\usepackage{amsthm}
\usepackage{csvsimple}
\usepackage{thmtools,thm-restate}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{multirow,bigdelim}
\usepackage{multicol}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{letltxmacro}
\usepackage{sansmath}
\usepackage{url}
\usepackage{flushend}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage{mathpartir}
\usepackage{empheq}
\usepackage{array}
\usepackage{pgfplots}
\usepackage{stmaryrd}
\usepackage{courier}
\usepackage{qtree}
\usepackage[normalem]{ulem}
\usepackage{relsize}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{stackengine}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{remreset}
\usepackage{tabulary}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{fullpage}
\usepackage[explicit]{titlesec}

% \renewcommand*\thesection{\arabic{section}.0}
% \renewcommand*\thesubsection{\arabic{section}.\arabic{subsection}}


\newtheorem*{theorem*}{Theorem}
\newenvironment{centermath}
 {\begin{center}$\displaystyle}
 {$\end{center}}
\setcellgapes{4pt}%parameter for the spacing

\lstset{ language=Caml, basicstyle=\upshape\sffamily,
keywordstyle=\upshape\sffamily\color{dkpurple}, keepspaces=true,
framexleftmargin=1ex, framexrightmargin=1ex, showstringspaces=true,
commentstyle=\itshape\rmfamily,
emph={rep,iterate,synth,collapse,perm,squash,normalize,using,ins,del,lens,let,get,put,rquot,lquot,id,swap,concat,or,disconnect,merge_left,merge_right,const},
emphstyle=\upshape\sffamily\color{dkpurple}, 
columns=fullflexible,
mathescape, 
xleftmargin=1.5em,
% BCP: I find this distracting:
stringstyle=\sffamily\color{dkblue},
}
\makeatletter
     \let\lst@oldvisiblespace\lst@visiblespace
     \def\lst@visiblespace{\,\lst@oldvisiblespace\,}
\makeatother

\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

\usetikzlibrary{
  er,
  matrix,
  shapes,
  arrows,
  positioning,
  fit,
  calc,
  pgfplots.groupplots,
  arrows.meta
}
\tikzset{>={Latex}}

%%%% Hyperlinks â€“ must come late!
%\usepackage[pdftex,%
%            pdfpagelabels,%
%            linkcolor=blue,%
%            citecolor=blue,%
%            filecolor=blue,%
%            urlcolor=blue]
%           {hyperref}

\input{macros}

\begin{document}

\pagestyle{empty}

\begin{center}

  \large \textbf{Synthesizing Data Wranglers\\\vspace{1cm} FA8750-17-2-0028}

  \vspace{1in}

  \normalsize
  %\begin{tabular}{rl}
  Kathleen Fisher (Tufts University) \\
  Benjamin Pierce  (University of Pennsylvania) \\
  David Walker (Princeton University) \\
  Steve Zdancewic (University of Pennsylvania)
 %\end{tabular}
  \vspace{1in}

  Final Report

\end{center}

\newpage
\pagestyle{plain}

\renewcommand{\thepage}{\roman{page}}% Roman numerals for page count
\setcounter{page}{1}% Start page number with 2

\tableofcontents
\newpage

\listoffigures
\newpage

\renewcommand{\thepage}{\arabic{page}}% Arabic numerals for page counter
\setcounter{page}{1}% Start page number with 2

\section{SUMMARY}



\section{INTRODUCTION}

Building information-processing systems and maintaining them over a long
period of time is a tedious, labor-intensive process.  One key challenge is
that such systems must often interact with a large number of
\emph{ad hoc data sources}---partially structured data sources represented
in non-standard formats.  Ad hoc data sources include various
different kinds of system log files as well as scientific data sources
generated by experiments.  These ad hoc data sources are often
produced by other automated systems, and each requires custom tools.
Over time, the data sources tend to evolve---fields are added,
removed, or co-opted, variants are added, etc.  Today, to manage these
changes, engineers must manually re-code parsers and/or insert
adaptors while maintaining the desired semantics.  Such manual work is
not only time-consuming, but exceedingly error-prone.  Moreover,
errors in environment-facing interfaces can not only lead to
corruption of important data, but also to significant security
vulnerabilities.  In order to design and implement survivable,
long-lived, complex software systems that are robust to changes 
in their operational environment---the vision of
DARPA's BRASS program---it is necessary to develop new, easier-to-use
and more robust programming systems for managing these ad hoc data
sources as they evolve.

To help alleviate this problem, we studied algorithms 
for automatically synthesizing adaptors between related data
sources, given (i) the \emph{type} of each
source, as well as (ii) a small collection of
representative \emph{examples} of the desired translation.  The
adaptors we synthesize are \emph{bidirectional}, meaning
that we synthesize transformations that may be applied both
backwards and forwards (from source A to B as well as B back to A).
Such bidirectional transformations may help faciliate evolution and
maintainence of long-lived systems by making it possible to upgrade
one component of a (possibly distributed) system, while it continues
to interact correctly with other components and with its environment.
The bidirectional transformations will be guaranteed to preserve
strong invertability laws, thereby reducing the likelihood of
inadvertant data corruption.  In addition, parsing components
will be synthesized by a compiler rather than being manually coded,
thereby reducing the likelihood of the buffer overruns that lead to
many security vulnerabilities.

The core result of our research is a new algorithm for
the synthesis of \emph{bijective string lenses}.  Bijective string lenses (\emph{i.e.,}
bidirectional transformations) define a limited set of
transformations between strings.  The domain and range of such
transformations are determined by regular expressions ({\emph{i.e.,}
  regular expressions serve as the types of these transformations).
 In addition, as their name suggests, these transformations are
 \emph{bijections}.
 In other words, the information content of source A is preserved (though usually
rearranged) when data is translated to target B, and vice versa when B
is translated back to A.  Such transformations can rearrange fields of
a record or insert new kinds of syntactic separators (\emph{e.g.,} replacing a
comma with a vertical bar, or the name of one HTML tag with another)
but they cannot implement more general transformations that elide
irrelevant details, such as the amount of whitespace that separates
two tokens.

Despite their limitations, bijections and bijective synthesis form a
useful foundation on top which more general transformations between
data sets may be built.  Hence, in the second half of our of project,
we extended the regular expression-based specifications of data types
with \emph{isomorphisms} between strings, also known as
\emph{quotients}.  To synthesize transformations between sets of
strings modulo isomorphisms, we generate canonizing functions followed
by bijections (using the original bijective synthesis algorithm).  The
so-called \emph{quotient lenses}~\cite{Foster:quotient-lenses}we generate are capable of handling
``irrelevant'' differences between structures such as white space or
permutations of items in a row of data set and hence expand the set of
transformations our system can define significantly.  Finally, we explored the
synthesis of \emph{symmetric lenses}~\cite{hofmann+:lenses} , again building upon the
bijective platform that we started with.  Symmetric lenses are able to
ignore arbitrary chunks of data in A when generating target data B,
and vice-versa.  For example, a serial number specific to one data set
may be ignored when we translate to a second.  Symmetric lenses
further expand the set of allowed transformations.

We measured the effectiveness of our algorithms on a set of
benchmarks drawn from the Augeas system~\cite{augeas}---a system for
transforming and editing Linux configuration files.  Past synthesis
tools, such as FlashFill~\cite{flashfill} were unable to translate
most Augeas file formats.  However, our synthesis toolkit was able to
transform all 40 of the Augeas file formats that we analyzed.

We have written three academic papers~\cite{?}
describing and evaluating our algorithms and produced open-source our code that
implements those algorithms.

\section{METHODS, ASSUMPTIONS, AND PROCEDURES}

Our approach combines two strategies.  We use \textit{domain-specific programming
  language}, specifically, the Boomerang~\cite{boomerang, Matching10} language,
which is designed for writing bidirectional string transformations.  We also exploit
\textit{type- and example-directed program synthesis}, which uses enumerative
techniques to search a space of candidate solutions.  The search is constrained
by the type information about the program, as well as by example instances
provided by the user.

\subsection{Lenses and lens domain-specific programming languages}

A lens comprises two functions,
\emph{get} and \emph{put}.  The \emph{get} function translates
source data into the target format.  If the target data is updated, the
\emph{put} function translates this edited data back into the
source format.  
A benefit of lens-based languages is that they use a single term
to express both 
\emph{get} and \emph{put}.
Furthermore, well-typed lenses give rise to 
\emph{get} and \emph{put} functions 
guaranteed to satisfy desirable invertibility properties.

Lens-based languages are present in variety of tools and have found mainstream
industrial use.
Boomerang~\cite{boomerang, Matching10} lenses provide
guarantees on transformations between {\em ad hoc} string document formats.
Augeas~\cite{augeas}, a popular tool that reads Linux system configuration
files, uses the \emph{get} part of a lens to transform configuration
files into a canonical tree representation that users can edit
either manually or 
programmatically.  It uses the lens's \emph{put} to merge the edited
results back into the original string format.  Other lens-based languages and
tools include 
%
GRoundTram~\cite{Hidaka2011GRoundTramAI},
%
BiFluX~\cite{DBLP:conf/ppdp/PachecoZH14}, 
%
BiYacc~\cite{DBLP:conf/staf/ZhuK0SH15},
%
Brul~\cite{DBLP:conf/etaps/ZanLKH16},
%
BiGUL \cite{DBLP:conf/pepm/KoZH16}, 
%
bidirectional variants of 
relational algebra~\cite{BohannonPierceVaughan},
spreadsheet formulas~\cite{DBLP:conf/vl/MacedoPSC14},
graph query languages~\cite{DBLP:conf/icfp/HidakaHIKMN10},
and
XML transformation languages~\cite{DBLP:conf/pepm/LiuHT07}.


\subsection{Bijective lens synthesis}

As inputs, our bijective synthesis procedure takes regular expressions specifying the
source, $S$, and
target, $T$, formats, plus a collection of concrete examples of the desired 
transformation.  Format specifications are supplied as ordinary regular
expressions.
Because regular expressions are so widely understood, we anticipate such
inputs will be substantially easier for everyday programmers to work with
than the unfamiliar syntax of lenses.
Moreover, these format descriptions communicate a
great deal of information to the synthesis system.  Thus, requiring user input
of regular expressions makes synthesis robust, 
helps the system scale to large and complex data sources, and 
constrains the search space sufficiently that the user typically needs
to give very few, if any, examples.

The goal of the synthesis algorithm is to find a lens
$\ell : S \Leftrightarrow T$ that corresponds to a bijection from the language
of a source regular expression $S$ to a target language of a target regular
expression $T$.   Such a lens should satisfy the \textit{bijective lens laws}:

\begin{equation}\label{bijectivelenslaws} \ell.\get \;
  (\ell.\lput \; t) = t \text{, and } \ell.\lput \; (\ell.\get \; s) = s
\end{equation}

\begin{figure}
  \centering
  \small 
  \begin{tikzpicture}[auto,node distance=1.5cm]
    \node[text width=1.5cm,minimum height=.6cm,align=center,draw,rectangle] (todnfregex) {\ToDNFRegex{}};
    
    \node[align=right, anchor=east] (regex1) [left = .6cm of todnfregex.north west]{\Regex{}};
    \node[align=right, anchor=east] (regex2) [left = .6cm of todnfregex.south west]{\RegexAlt{}};
    \node[align=right, anchor=east] (exs) [below = .2cm of regex2]{ \Examples{} };
    
    \node[align=center] (dnfregex1) [right = .4cm of todnfregex.north east]{\DNFRegex{}};
    \node[align=center] (dnfregex2) [right = .4cm of todnfregex.south east]{\DNFRegexAlt{}};

    \node[text width=2.3cm,minimum height=.6cm,align=center,draw,rectangle] [right = 1.45cm of todnfregex.east] (synthdnflens) {\SynthDNFLens{}};
    \node[align=center] [above = .7cm of synthdnflens] (optician) {\Optician{}};
    
    \node[align=center] [right = .4cm of synthdnflens] (dnflens) {\DNFLens{}};
    
    \node[text width=1.5cm,minimum height=.6cm,align=center,draw,rectangle] [right = .4cm of dnflens] (tolens) {\ToLens{}};
    
    \node[align=center] [right = .6cm of tolens] (lens) {\Lens{}};
    
    
    \path[->] (regex1.east) edge (todnfregex.north west);
    \path[->] (regex2.east) edge (todnfregex.south west);
    
    \path[->] (todnfregex.north east) edge (dnfregex1.west);
    \path[->] (todnfregex.south east) edge (dnfregex2.west);
    
    \path[->] (dnfregex1.east) edge (synthdnflens.north west);
    \path[->] (dnfregex2.east) edge (synthdnflens.south west);
    
    \path[->] (synthdnflens) edge (dnflens);
    
    \path[->] (dnflens) edge (tolens);
    
    \path[->] (tolens) edge (lens);
    \draw[->] ($(exs.east)+(-3pt,0)$) -| node(exsedge) {} (synthdnflens);
    \node[fit={($(todnfregex.west)+(-4pt,0)$) ($(tolens.east)+(4pt,0)$) (exsedge) (dnfregex1) (optician) (dnfregex2)},draw] (surrounding) {};
    % Now place a relation (ID=rel1)
    %\node[text width=2cm,align=center,draw, rectangle] (sketch-gen) [right = .75cm of spec] {\TypeProp{}};
    %\node (below-gen) [below=.5cm of sketch-gen] {};
    %\node[text width=2cm,align=center,draw, rectangle] (sketch-compl)
    %     [right = .25cm of sketch-gen] {\RigidSynth{}};
    %\node (below-compl) [below=.5cm of sketch-compl] {};
    %\node[align=center] (lens) [right = .75cm of sketch-compl] {Lens}; 
    %% Draw an edge between rel1 and node1; rel1 and node2
    %\path[->] (spec) edge node (start-alg) {} (sketch-gen);
    %\path[->] (sketch-gen) edge node(middle) {} (sketch-compl);
    %\path[->] (sketch-compl) edge node[near start](success) {\Success{}} (lens);
    %\draw[<-] (sketch-gen.south) -- +(0,-.5) -| node[above left](failure){\Failure{}} (sketch-compl.south);

    %\node (synth-name) [above=.5cm of middle] {\Optician{}};
    %
    %\node[fit=(sketch-gen) (sketch-compl) (start-alg) (synth-name) (failure)
    %(success) ,draw] (surrounding) {};

  \end{tikzpicture}
  \caption{Schematic Diagram for \Optician{}.  Regular expressions, \Regex{} and
    \RegexAlt{}, and examples, \Examples{}, are given as input.
    First, the function \ToDNFRegex{} converts \Regex{} and \RegexAlt{} into
    their respective DNF forms, \DNFRegex{} and \DNFRegexAlt{}.
    Next, \SynthDNFLens{} synthesizes a DNF lens, \DNFLens{}, from \Regex{},
    \RegexAlt{}, and \Examples{}.
    Finally, \ToLens{} converts \DNFLens{} into \Lens{}, a lens in Boomerang
    that is equivalent to \DNFLens{}.}
  \label{fig:schematic-diagram-synthesis}
\end{figure}

Figure~\Cref{fig:schematic-diagram-synthesis} shows a high-level,
schematic diagram for \Optician{}.
First, \Optician{} uses the function \ToDNFRegex{} to convert the input
regular expressions into DNF regular expressions.  Next,
\SynthDNFLens{} performs type-directed synthesis on these DNF regular
expressions and the input examples to synthesize a DNF lens.  Finally, this DNF lens is
converted back into a regular lens with the function \ToLens{}, and returned to the user.


\subsection{Quotient lens synthesis}

Quotient lenses are lenses in which the lens laws are
loosened so that they hold modulo an equivalence relation on the source and
target data respectively; as above, we are concerned with {\em bijective
quotient lenses} which are lenses for which Equation \ref{bijectivelenslaws}
holds modulo equivalence relations $\equiv_S$ and $\equiv_T$ defined on the source
and target data respectively:

\begin{equation}\label{quotientlenslaws}
\ell.\get \; (\ell.\lput \; t) \equiv_T t \text{, and } \ell.\lput \; (\ell.\get
\; s) \equiv_S s
\end{equation}

The inputs to the quotient lens synthesis problem, as in the bijective case,
include source and target regular expressions $S$ and $T$ and a set of example
instances.  The equivalence relations $\equiv_S$ and $\equiv_T$ are also given
as inputs to the synthesis algorithm.  As we describe below, one result of our
work is a language of \textit{quotient regular expressions} that concisely
specifies $S$ and $\equiv_S$ simultaneously.

\section{RESULTS AND DISCUSSION}

% From bijective lens paper


% \newcommand{\SOptician}{Optician\textsubscript{S}}
% \newcommand{\SynthSymLens}{\PCF{SynthSymLens}\xspace}
% \newcommand{\RXSearchState}{\ensuremath{\mathit{pq}}}
% \newcommand{\ToStochastic}{\PCF{ToStochastic}\xspace}
% \newcommand{\LC}{\ensuremath{\mathit{lc}}}
% \newcommand{\Best}{\ensuremath{\mathit{best}}}
% \newcommand{\RXSearch}{\PCF{RXSearch}\xspace}
% \newcommand{\Continue}{\PCF{Continue}\xspace}
% \newcommand{\PQ}{\PCF{PQ}\xspace}
% \newcommand{\SynthDNFLens}{\PCF{SynthDNFLens}}
% \newcommand{\ToLens}{\ensuremath{\Uparrow}}
% \newcommand{\ToLensOf}[1]{\ensuremath{\ToLens{}\mkern-4mu #1}}
% \newcommand{\ToDNFRegexText}{\PCF{ToDNFRegex}}
% \newcommand{\Beautify}{\PCF{Beautify}}
% \newcommand{\RigidSynth}{\PCF{RigidSynth}}
% \newcommand{\GreedySynth}{\PCF{GreedySynth}\xspace}
% \newcommand{\RigidSynthInternal}{\PCF{RigidSynthInternal}}
% \newcommand{\RigidSynthSequence}{\PCF{RigidSynthSeq}}
% \newcommand{\RigidSynthAtom}{\PCF{RigidSynthAtom}}
% \newcommand{\GetDNFNormalizer}{\PCF{GetDNFNormalizer}}
% \newcommand{\CreatePQueue}{\PCF{CreatePQueue}}
% \newcommand{\GetTransitiveSet}{\PCF{GetTransitiveSet}}
% \newcommand{\GetCurrentSet}{\PCF{GetCurrentSet}}
% \newcommand{\Pop}{\PCF{Pop}}
% \newcommand{\ExpandOnce}{\PCF{ExpandOnce}}
% \newcommand{\ExpandRequired}{\PCF{ExpandRequired}}
% \newcommand{\FixProblemElts}{\PCF{FixProblemElts}}
% \newcommand{\Expand}{\PCF{Expand}\xspace}
% \newcommand{\ForceExpand}{\PCF{ForceExpand}}
% \newcommand{\Reveal}{\PCF{Reveal}}
% \newcommand{\Map}{\PCF{Map}}
% \newcommand{\EnqueueMany}{\PCF{EnqueueMany}}
% \newcommand{\ReturnVal}[1]{\ensuremath{\Return\,#1}}
% \newcommand{\CurrentSet}{\ensuremath{\mathit{CS}}}
% \newcommand{\TransitiveSet}{\ensuremath{\mathit{TS}}}

% \newcommand{\SSOpt}{\ensuremath{\mathbf{SS}}}
% \newcommand{\SSNCOpt}{\ensuremath{\mathbf{SSNC}}}
% \newcommand{\BSOpt}{\ensuremath{\mathbf{BS}}}
% \newcommand{\BSNCOpt}{\ensuremath{\mathbf{BSNC}}}
% \newcommand{\AnyOpt}{\ensuremath{\mathbf{Any}}}
% \newcommand{\FLOpt}{\ensuremath{\mathbf{FL}}}
% \newcommand{\CCOpt}{\ensuremath{\mathbf{DC}}}
% \newcommand{\NSOpt}{\textbf{NS}}
% \newcommand{\NROpt}{\textbf{NR}}

We have implemented our synthesis algorithms in
of OCaml and integrated them into Boomerang system,
where they are available as open source code~\cite{GitHub}.
Users of Boomerang can write lenses by hand, specify them as
synthesis tasks, or do a combination of both.

We have analyzed our algorithms theoretically, proving various
soundness and completeness results.  We have also analyzed our
algorithms empirically, studying their impact on a range of real and
synthetic benchmarks.  Our full results have been published in a
series of conference papers and technical
reports~\cite{bijective,quotient,symmetric}.  In the rest of this
section, we summarize some of the key results.

\subsection{Bijective Synthesis Algorithms}

We first study the effectiveness of the pure bijective synthesis
algorithm by evaluating its performance on a set of 39
benchmark programs.  

All evaluations were performed on a 2.5 GHz Intel Core i7 processor with 16 GB
of 1600 MHz DDR3 running macOS Sierra.

\paragraph*{Benchmark Suite Construction}
We constructed our benchmarks primarily by adapting examples from
Augeas~\cite{augeas} and 
Flash Fill~\cite{gulwani-popl-2014}.
%
Augeas is a configuration editing system for Linux that uses lens
combinators similar to those in Boomerang. However, it transforms
strings on the left to structured trees on the right rather than
transforming strings to strings.
We adapted these Augeas lenses to our setting by converting the
right-hand sides to strings that correspond to serialized versions
of the tree formats.  
We derived 29 of the benchmark tests by
adapting the first 27 lenses in alphabetical order, as well as the lenses
\CF{aug/xml-firstlevel} and \CF{aug/xml} that were referenced
by the `A' lenses.
Furthermore, the 12 last synthesis problems derived
from Augeas were tested after \Optician{} was
finalized, demonstrating that the optimizations were not
overtuned to perform well on the testing data.
%
Flash Fill is a system that allows users to specify common string
transformations by example~\cite{gulwani-popl-2014}.  
We derived three benchmarks from the first few examples in the
paper and one from the running example on
extracting phone numbers.

Both Augeas and Flash Fill permit non-bijective transformations.
To test our system on these benchmarks, we had to convert them into
bijective transformations by hand.  More specifically, in many of the
benchmarks, we normalized
the whitespace in the documents so the amount of whitespace was equal
in the source and target.  In a few of the benchmarks, either the
source or the target was missing information present in the other
format.  We modified those benchmarks by adding the missing
information back into the file. 

Finally, we added custom examples to highlight weaknesses of
our algorithm (\CF{cap-prob} and \CF{2-cap-prob}) 
and to test situations for which we thought the tool would be
particularly useful (\CF{workitem-probs}, \CF{date-probs}, \CF{bib-prob},
and \CF{addr-probs}).   These examples convert between work item formats, date
formats, bibliography formats, and address formats, respectively.

\begin{figure}
  \centering
  \begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics{figs/specsizes}
    \caption{}
    \label{subfig:lenssize}
  \end{subfigure}
  \begin{subfigure}[b]{.49\textwidth}
    \includegraphics{figs/examplesused}
    \caption{}
    \label{subfig:examplesused}
  \end{subfigure}
  \caption{Sizes of Specifications.
    In (a), we show how many benchmarks are defined in our suite using a
    given number of AST nodes or fewer.
    In (b), we show how many benchmarks are defined in our suite using a
    given number of examples or fewer.}
  \label{fig:definition-sizes}
\end{figure}

Figure~\ref{fig:definition-sizes} shows the complexity of our type-based
specifications as well as our example counts.
An average benchmark has a type-based specification that can be represented using
310 AST node, and requires 1.1 input/output examples.
Our benchmarks vary from simple problems, like changing the
representation of dates
(with a specification size of 85, and a generated lens size of 79), to 
complex tasks, like transforming configuration files for server monitoring
software into dictionary form (with a specification size of 670 and
a generated lens size of 651).  On average, the size of the generated lens is
89\% the size of its type specifications.


\paragraph*{Importance of Examples}

\begin{figure}
  \centering
  \includegraphics{figs/examples.eps}
  \caption{Average number of random examples required to synthesize benchmark
    programs.  {\bf Experimental Average} is the average number of randomly
    generated examples needed to correctly synthesize the lens.  {\bf
      Determinize Permutations} is the theoretical number of examples required
    to determinize the choice all the permutations in \RigidSynth{}.
    In practice, far fewer examples are
    needed to synthesize the correct lens than would be predicted by the number
    required to determinize permutations.}
  \label{fig:exs-reqd}
\end{figure}

To evaluate how many user-supplied examples the algorithm requires in
practice, we \textit{randomly} generated appropriate source/target
pairs, mimicking what a na\"{i}ve user might do.  We did not write the
examples by hand out of concern that our knowledge of the synthesis
algorithm might bias the selection. Figure~\ref{fig:exs-reqd} shows
the number of randomly generated examples it takes to synthesize the
correct lens averaged over ten runs.  The synthesis algorithm almost never needs
any examples: only 5 benchmarks need a nonzero number of examples to
synthesize the correct lens and only one, \CF{cust/workitem-probs} required over
10 randomly generated examples.
A clever user may be able to reduce the
number of examples further by selecting examples carefully; we
synthesized \CF{cust/workitem-probs} with only 8 examples.

These numbers are low because there are relatively few well-typed
bijective lenses between any two source and target regular expressions. 
As one would expect, the benchmarks where there are multiple ways to
map source data to the target (and vice versa) require the most examples.
For example, the benchmark \CF{cust/workitem-probs} requires a large number of
examples because it
must differentiate between data in different text fields in both the
source and target and map between them appropriately.  As these text fields are
heavily permuted
(the legacy format ordered fields by a numeric ID, where
the modern format ordered fields alphabetically) and fields can be
omitted, a number of examples are needed to correctly identify the mapping
between fields.

The average number of examples to
infer the correct lens does not tell the whole story.  The system will
stop as soon as it finds a well typed lens that satisfies the supplied examples.
This inferred lens may or may not 
correctly handle unseen examples that correspond to
unexercised portions of the source and target regular expressions.
Figure~\ref{fig:exs-reqd} lists
the number of examples that are required to determinize the generation of
permutations in \RigidSynth{}.
Intuitively, this number represents the maximum number of
examples that a user must supply to guide the synthesis engine if it
always guesses the wrong permutation when multiple permutations can be used to
satisfy the specification. 

The average number of examples is so much lower than the maximum
number of required examples because of correspondences in how we wrote
the regular expressions for the source and target data formats. 
Specifically, when we had corresponding disjunctions in both the
source and the target, we ordered them the same way.  The algorithm
uses the supplied ordering to guide its search, and so the system
requires fewer examples.   We did not write the examples in this style
to facilitate synthesis, but rather because maintaining similar
subparts in similar orderings makes the types much easier to 
read. We expect that most users would do the same.

\paragraph*{Comparison Against Other Tools}
%
We are the first tool to synthesize bidirectional transformations between data
formats, so there is no tool to which we can make an apple-to-apples comparison.
Instead, we compare against tools for generating unidirectional
transformations instead. 
Figure~\ref{fig:synthesis-times} includes a comparison against two other
well-known tools that synthesize
text transformation and extraction functions from examples -- Flash Fill and FlashExtract.  For this
evaluation, we used the version of these tools distributed through the
PROSE project~\cite{prose}.

\begin{figure}
  \includegraphics{figs/times}
  \caption{
    Number of benchmarks that can be solved by a given algorithm in a given
    amount of time.
    \Optician{} is our bijective synthesis algorithm including a set
    of optimizations we have implemented.
    \FlashExtractMode{} is the existing FlashExtract system.  \FlashFillMode{} is
    the existing Flash Fill system.  \NaiveMode{} is na\"{i}ve type-directed 
    synthesis on the bijective lens combinators.  Our synthesis algorithm performs
    better than the na\"{i}ve approach and other string transformation systems,
    and our optimizations speed up the algorithm enough that all tasks become
    solvable.
  }
  \label{fig:synthesis-times}
\end{figure}

To generate specifications for Flash Fill, we generated input/output
specifications by generating random elements of the source language, and
running the lens on those elements to generate elements of the target language.
These were then fed to Flash Fill.

To generate specifications for FlashExtract, we extracted portions of strings
mapped in the generated lens either through an identity
transformation or through a previously synthesized lens, whereas strings that were
mapped through use of \ConstLens{} were considered boilerplate and so not
extracted.

As these tools were designed for a broader audience, they put less of a burden
on the user.  These tools only use input/output examples (for Flash
Fill), or marked text regions (for FlashExtract), as opposed
to \Optician{}'s use of regular expressions to constrain the format of
the input and output.  By using regular expressions,
\Optician{} is able to synthesize significantly more programs
than either existing tool.

Flash Fill and FlashExtract have two tasks: to determine how
the data is transformed, they must also infer the structure of the data, a
difficult job for complex formats.
In particular, neither Flash Fill nor FlashExtract was able to synthesize
transformations or extractions present under two iterations, a type of format
that is notoriously hard to infer.
These types of dual iterations are pervasive in Linux configuration
files, making Flash Fill and FlashExtract ill suited for many of the synthesis
tasks present in our test suite.

Furthermore, as unidirectional transformations, Flash Fill and FlashExtract have
a more expressive calculus.  To guarantee bidirectionality, our syntax must be
highly restrictive, providing a smaller search space to traverse.

\subsection{Quotient Synthesis Algorithms}

% DPW:  From Quotient lenses paper

\newcommand{\wf}[1]{\ensuremath{#1\;\mathsf{wf}}}

% FOR Regular Expression names
\newcommand{\re}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\codefont}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\kw}[1]{\textcolor{dkblue}{\ensuremath{\mathsf{#1}}}}
\newcommand{\collapse}[2]{\ensuremath{\kw{collapse} \; #1 \mapsto #2}}
\newcommand{\squash}[3]{\ensuremath{\kw{squash} \; #1 \rightarrow #2\; \kw{using} \; #3}}
\newcommand{\perm}[2]{\ensuremath{\kw{perm}(#1)\; \kw{with}\; #2}}
\newcommand{\normalize}[3]{\ensuremath{\kw{normalize}(#1, #2, #3)}}
\newcommand{\eqrel}[1]{\ensuremath{\equiv_{#1}}}

\newcommand{\canonize}{\ensuremath{\kw{canonize}}}

\newcommand{\Name}{Optometrist\xspace}

\newcommand{\QRESize}{\textbf{QS}}
\newcommand{\canonizeAndSpecSize}{\textbf{BS}}
\newcommand{\LensAndSpecSize}{\textbf{NS}}

%\newcommand{\QOpt}{Optician_Q}
\newcommand{\QOpt}{QRE-enhanced Optician}
\newcommand{\OpticianRuntime}{\textbf{Optician}}
\newcommand{\QREOptician}{\textbf{Optician\textsubscript{Q}}}
\newcommand{\SystemOnOptician}{\textbf{QO}}
\newcommand{\SystemOnBenchmarks}{\textbf{QQ}}
\newcommand{\cd}[1]{\lstinline[backgroundcolor=\color{white}]$#1$}

We have implemented QREs and the quotient lens synthesis algorithm described
above as an extension to the Boomerang
interpreter~\cite{boomerang,quotientlenses}. We have extended the existing
Optician tool to synthesize QRE lenses from QREs. We will use 
``\QOpt'' to denote our extended version of Optician, and just plain ``Optician'' to denote
the pre-QRE version of Optician. The synthesis algorithm produces Boomerang lens
values, so Boomerang gives synthesized lenses the same first-class status as hand-written ones. To
evaluate the effectiveness of QREs, QRE lenses, and QRE lens synthesis, we
conducted experiments to answer the following questions:
%
\begin{itemize}
  \item {\bf Ease of use.} Does synthesizing QRE lenses from QREs permit an
  easier development process than writing lenses by hand?
  Does synthesizing QRE lenses from QREs permit an easier development
  process than manually writing canonizers and then synthesizing lenses between
  their canonized forms?  
  \item {\bf Performance.} Is the synthesis algorithm/implementation fast enough to be
  used as part of a standard development process?
\end{itemize}
%
All evaluations were performed on a 2.5 GHz Intel Core i7 processor with 16 GB
of 1600 MHz DDR3 running macOS High Sierra.


\paragraph*{Benchmark Suite Construction}

To evaluate our QRE implementation, we adapted 39 lens synthesis tasks
from the benchmark suite of the original Optician system. These
benchmarks are a combination of custom benchmarks, benchmarks derived
from FlashFill~\cite{flashfill}, and benchmarks derived from
Augeas~\cite{augeas2} (we also experimented using our QRE implementation to synthesize quotient lenses between XML, RDF and and JSON formats using data from the data.gov database; the data consisted of census statistics, demographic statistics, wage comparosion data, and crime index data).  In these 39 benchmarks, 10 of
the data formats had to be modified to work with the bijectivity
constraints that Optician required. For instance, when one
representation permits whitespace where the other does not, we
modified the original version of the benchmark to allow more whitespace,
thereby restoring bijectivity (but altering the data format). With the
new QRE support, we were able to remove these alterations.  This
experience alone suggests that QREs make the lens development process
more flexible.

\paragraph*{Ease of Use}

\begin{figure}[t]
\includegraphics{qfigs/asts.eps}
\caption{AST node measurements for each of the three approaches
on each of the 10 non-bijective benchmark problems.
Benchmarks are sorted in order of increasing complexity as measured by
the number of AST nodes in the source and target format descriptions. 
QRE Synthesis requires far fewer AST nodes than the other
two approaches.}
%%\caption{Count of benchmark programs definable using a given AST count. We find
%%that it takes far fewer AST nodes to define benchmark lenses using QRE
%%synthesis than with Optician without QRE synthesis or without synthesis.}
\label{fig:asts}
\end{figure}

To evaluate the impact of QRE lens synthsis on programmer effort,
we focus our attention on the 10 problems in the benchmark suite that
are not bijective and hence require non-trivial canonizers. 
(Optician already handles the other problems with minimal programmer
effort.)

We are interested in comparing three different approaches, which vary
in the amount of synthesis used. 
In the first approach, which we call \QRESize{} for QRE Synthesis, the programmer uses
QRE lens synthesis.  She must write QRE specifications of the source and
target formats and she may give examples.
In the second approach, which we call \canonizeAndSpecSize{} for
Bijective Synthesis, the
programmer uses bijective lens synthesis \`a la Optician.
She must write canonizers by hand, along with 
regular expressions to describe the external
representations of the source and target formats. (The internal
formats can be inferred from the canonizers.) She may also
provide examples to help in the synthesis of the bijective lens.
In the third approach, which we call \LensAndSpecSize{} for No Synthesis, the
programmer writes the lens between the source and target formats
entirely by hand, including the descriptions of the source and target
formats.

For each problem in the benchmark suite, we calculate the following
measures as proxies for the level of programmer effort when using each
the three approaches:

%
\begin{itemize}
  \item[\QRESize{}:] 
  The number of AST nodes in the QRE specifications for the source and
  target formats, including examples. 
  \item[\canonizeAndSpecSize{}:] 
  The sum of (1) the number of AST nodes in $W(q)$ for each QRE $q$ in the source and target
  formats, (2) the number of AST nodes in $\canonize(q)$ for each QRE $q$ with a
  non-trivial canonizer, and (3) the number of AST nodes in the
  examples.  We use (1) to estimate the burden of describing
  the external source and target formats and (2) to estimate the
  burden of writing the requisite canonizers
  by hand.  We count the nodes in the examples because they would be
  fed to the bijective synthesizer.  
  These counts are an approximation, as both $W(q)$ and $\canonize(q)$ are
  automatically generated from the corresponding QRE $q$, and it is
  possible that a human-written version might be smaller.
  \item[\LensAndSpecSize{}:] The sum of (1) the number of AST nodes in
  $W(q)$ for each QRE $q$ in the source and target formats and (2) the
  number of AST nodes in the synthesized QRE lens.  We use (1) to
  estimate the burdern of describing the source and target formats
  and (2) to estimate the burdern of writing the appropriate lens by
  hand. These counts are also approximations, as
  $W(q)$ and the synthesized lens may be larger than one written by hand.
\end{itemize}

Figure~\ref{fig:asts} shows each of these measures for the 10
non-bijective problems in the benchmark suite.  On
average (using a geometric mean), \canonizeAndSpecSize{} used~38.5\% more AST nodes
than \QRESize{}, requiring an average of~214 more AST nodes. On 
average, \LensAndSpecSize{} used~180\% more AST nodes than \QRESize{}, requiring an
average of~998 more AST nodes. These figures suggest that introducing QREs saves
programmers significant effort compared to both Optician and basic
Boomerang.

\paragraph*{Maintaining Competitive Performance}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.49\textwidth}
\centering
\includegraphics{qfigs/times_opt}
\caption{}
\label{subfig:lenssize}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\includegraphics{qfigs/times_new.eps}
\caption{}
\label{subfig:examplesused}
\end{subfigure}
\caption{Runtimes measurements. In (a), we run Optician and \QOpt{}
  on the Optician benchmarks.  We find that there is only a negligible
  performance overhead incurred by using QREs. 
  In (b), we run \QOpt{} on the 10 Optician benchmarks
  previously edited to make them bijective, after removing those edits
  and then extending the synthesis specification to include QREs.
  (In other words, we restored them to their original state, added QREs,
  and then ran \QOpt{}). 
  We find that \QOpt{} is able to
  synthesize all quotient lenses in under 10 seconds, and typically finishes in
  under 5 seconds.}
\label{fig:times}
\end{figure}

To assess the performance of QRE synthesis, we are interested in two different
questions. First, how does the performance of \QOpt{} compare to the performance
of Optician on benchmarks that do not require QREs? The answer to this question
tells us how much overhead we have introduced by adopting the more general
mechanism. Figure~\ref{fig:times}(a) shows that \QOpt{} was able to synthesize
all of the Optician benchmarks at a speed competitive with the old version.
There is a small amount of additional overhead introduced by QREs in calculating
the $W$ and $K$ functions, resulting in a slight decrease in performance.

Second, how much time does it take for \QOpt{} to synthesize a QRE
lens when running on a non-bijective benchmark problem?  
Figure~\ref{fig:times}(b) shows the amount of time required to infer a
lens for each of the 10 benchmark programs with nontrivial quotients.  
We find that \QOpt{} is able to synthesize all quotient lenses in
under 10~seconds, and typically finishes in under 5~seconds.


\section{CONCLUSIONS}

During the course of this project, we have designed, analyzed and implemented algorithms that demonstrate it is possible to synthesize three classes of bidirectional transformations:  (1) pure bijective transformations, (2) bijections modulo equivalences classes and (3) bijections modulo projections.  The synthesis algorithms we have designed take inputs that include a format specification (which includes specification of equivalence classes) and a collection of examples.  As the class of transformations becomes richer, the number of potential programs grows dramatically.  As a result, the corresponding inference algorithm slows and its ability to guess the transformation desired by the user decreases, leading to reduced accuracy.  We found that it was possible to overcome such challenges through new heuristics that use information theory to help guide the search for program transformations.  In addition, we found the use of compositional synthesis, which is the process of breaking down a complex synthesis task into smaller, more mangeable subtasks critical when attempting to scale our prototype system up to be able to handle larger transformation task.

\bibliographystyle{apalike}
\bibliography{local.bib,bcp.bib}

\section{List of Symbols, Abbreviations, and Acronyms}
\begin{tabular}{ll}
  DARPA & Defense Advanced Research Project Agency \\
\end{tabular}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
