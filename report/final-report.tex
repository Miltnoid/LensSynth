\documentclass[12pt]{article}

\newif\ifdraft\drafttrue  % set true to show comments
% \newif\ifdraft\draftfalse  % set true to show comments
\newif\ifanon\anonfalse    % set true to suppress names, etc.
\newif\ifappendices\appendicesfalse

%\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage[capitalise]{cleveref}
\usepackage{makecell}%To keep spacing of text in tables
\usepackage{nccmath}
\usepackage{mathtools}
\usepackage{bussproofs}
\usepackage{varwidth}
\usepackage{amsthm}
\usepackage{csvsimple}
\usepackage{thmtools,thm-restate}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{multirow,bigdelim}
\usepackage{multicol}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{letltxmacro}
\usepackage{sansmath}
\usepackage{url}
\usepackage{flushend}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage{mathpartir}
\usepackage{empheq}
\usepackage{array}
\usepackage{pgfplots}
\usepackage{stmaryrd}
\usepackage{courier}
\usepackage{qtree}
\usepackage[normalem]{ulem}
\usepackage{relsize}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{stackengine}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{remreset}
\usepackage{tabulary}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{fullpage}
\usepackage[explicit]{titlesec}

% \renewcommand*\thesection{\arabic{section}.0}
% \renewcommand*\thesubsection{\arabic{section}.\arabic{subsection}}


\newtheorem*{theorem*}{Theorem}
\newenvironment{centermath}
 {\begin{center}$\displaystyle}
 {$\end{center}}
\setcellgapes{4pt}%parameter for the spacing

\lstset{ language=Caml, basicstyle=\upshape\sffamily,
keywordstyle=\upshape\sffamily\color{dkpurple}, keepspaces=true,
framexleftmargin=1ex, framexrightmargin=1ex, showstringspaces=true,
commentstyle=\itshape\rmfamily,
emph={rep,iterate,synth,collapse,perm,squash,normalize,using,ins,del,lens,let,get,put,rquot,lquot,id,swap,concat,or,disconnect,merge_left,merge_right,const},
emphstyle=\upshape\sffamily\color{dkpurple}, 
columns=fullflexible,
mathescape, 
xleftmargin=1.5em,
% BCP: I find this distracting:
stringstyle=\sffamily\color{dkblue},
}
\makeatletter
     \let\lst@oldvisiblespace\lst@visiblespace
     \def\lst@visiblespace{\,\lst@oldvisiblespace\,}
\makeatother

\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

\usetikzlibrary{
  er,
  matrix,
  shapes,
  arrows,
  positioning,
  fit,
  calc,
  pgfplots.groupplots,
  arrows.meta
}
\tikzset{>={Latex}}

%%%% Hyperlinks – must come late!
%\usepackage[pdftex,%
%            pdfpagelabels,%
%            linkcolor=blue,%
%            citecolor=blue,%
%            filecolor=blue,%
%            urlcolor=blue]
%           {hyperref}

\input{macros}

\begin{document}

\pagestyle{empty}

\begin{center}

  \large \textbf{Synthesizing Data Wranglers\\\vspace{1cm} FA8750-17-2-0028}

  \vspace{1in}

  \normalsize
  %\begin{tabular}{rl}
  Kathleen Fisher (Tufts University) \\
  Benjamin Pierce  (University of Pennsylvania) \\
  David Walker (Princeton University) \\
  Steve Zdancewic (University of Pennsylvania)
 %\end{tabular}
  \vspace{1in}

  Final Report

\end{center}

\newpage
\pagestyle{plain}

\renewcommand{\thepage}{\roman{page}}% Roman numerals for page count
\setcounter{page}{1}% Start page number with 2

\tableofcontents
\newpage

\listoffigures
\newpage

\renewcommand{\thepage}{\arabic{page}}% Arabic numerals for page counter
\setcounter{page}{1}% Start page number with 2

\section{Summary}

Maintaining software systems that manage evolving representations of
data is tedious and error prone.  In this study, we have developed
algorithms for automatically synthesizing \emph{lenses}, which are
software adaptors that convert between two different representations
of the same information.  Lenses are a special form of program that
guarantee so-called \emph{round-tripping properties} that ensure the
transformations are well-behaved.  Our synthesis algorithms take as
input (i) the \emph{type} of each source as a regular expression, as
well as (ii) a small collection of representative \emph{examples} of
the desired translation.  They produceas output \emph{bijective string
  lenses}.  Bijective string lenses are a special class of lenses that
only exist when the two data sources are related via a bijection.
Despite this limitation, bijections and bijective synthesis form a
useful foundation on top of which more general transformations between
data sets may be built.  We explore two of these extensions: quotient
lenses and symmetric lenses.  Quotient lenses apply when two formats
are ``morally'' but not technically bijective.  Examples include two
formats that differ in their use of white space or in the order of
sequences of elements.  Symmetric lenses allow one side or the other to have
information not present in the other.  When converting between the
formats, defaults take the place of missing information. 

We measured the effectiveness of our
algorithms on a set of 39 benchmarks drawn from Augeas~\cite{augeas},
which is a system for transforming and editing Linux configuration
files, and from FlashFill~\cite{flashfill}, which is an extension to
Microsoft Excel for synthesizing columns of spreadsheets.
On these benchmarks, we find it is possible to synthesize lenses of
comparable quality to hand-written lenses from format descriptions and
a very small number of examples, leveraging 
the large amount of information embedded in the format descriptions. 
We further found that as the formats diverge and the class of lenses
between them becomes richer, the synthesis problem becomes more
difficult and additional heuristics need to be considered.

We have written
papers~\cite{bijective-synthesis,quotient-synthesis,symmetric-synthesis}
describing and evaluating our algorithms. We produced open-source 
code~\cite{GitHub} that implements those algorithms.  In this report,
we summarize our key methods, results, and findings, focusing
on the core algorithms for bijective synthesis and quotient synthesis.



\section{Introduction}

Building information-processing systems and maintaining them over a long
period of time is a tedious, labor-intensive process.  One key challenge is
that such systems must often interact with a large number of
\emph{ad hoc data sources}---partially structured data sources represented
in non-standard formats.  Ad hoc data sources include various
different kinds of system log files as well as scientific data sources
generated by experiments.  These ad hoc data sources are often
produced by other automated systems, and each requires custom tools.
Over time, the data sources tend to evolve---fields are added,
removed, or co-opted, variants are added, etc.  Today, to manage these
changes, engineers must manually re-code parsers and/or insert
adaptors while maintaining the desired semantics.  Such manual work is
not only time-consuming, but exceedingly error-prone.  Moreover,
errors in environment-facing interfaces can not only lead to
corruption of important data, but also to significant security
vulnerabilities.  In order to design and implement survivable,
long-lived, complex software systems that are robust to changes 
in their operational environment---the vision of
DARPA's BRASS program---it is necessary to develop new, easier-to-use
and more robust programming systems for managing these ad hoc data
sources as they evolve.

To help alleviate this problem, we studied algorithms 
for automatically synthesizing adaptors between related data
sources, given (i) the \emph{type} of each
source, as well as (ii) a small collection of
representative \emph{examples} of the desired translation.  The
adaptors we synthesize are \emph{bidirectional}, meaning
that we synthesize transformations that may be applied both
backwards and forwards (from source A to B as well as B back to A).
Such bidirectional transformations may help faciliate evolution and
maintainence of long-lived systems by making it possible to upgrade
one component of a (possibly distributed) system, while it continues
to interact correctly with other components and with its environment.
The bidirectional transformations will be guaranteed to preserve
strong invertability laws, thereby reducing the likelihood of
inadvertant data corruption.  In addition, parsing components
will be synthesized by a compiler rather than being manually coded,
thereby reducing the likelihood of the buffer overruns that lead to
many security vulnerabilities.

The core result of our research is a new algorithm for
the synthesis of \emph{bijective string lenses}.  Bijective string lenses (\emph{i.e.,}
bidirectional transformations) define a limited set of
transformations between strings.  The domain and range of such
transformations are determined by regular expressions ({\emph{i.e.,}
  regular expressions serve as the types of these transformations).
 In addition, as their name suggests, these transformations are
 \emph{bijections}.
 In other words, the information content of source A is preserved (though usually
rearranged) when data is translated to target B, and vice versa when B
is translated back to A.  Such transformations can rearrange fields of
a record or insert new kinds of syntactic separators (\emph{e.g.,} replacing a
comma with a vertical bar, or the name of one HTML tag with another)
but they cannot implement more general transformations that elide
irrelevant details, such as the amount of whitespace that separates
two tokens.

Despite their limitations, bijections and bijective synthesis form a
useful foundation on top of which more general transformations between
data sets may be built.  Hence, in the second half of our of project,
we extended the regular expression-based specifications of data types
with \emph{isomorphisms} between strings, also known as
\emph{quotients}.  To synthesize transformations between sets of
strings modulo isomorphisms, we generate canonizing functions followed
by bijections (using the original bijective synthesis algorithm).  The
so-called \emph{quotient lenses}~\cite{quotientlenses} we generate are capable of handling
``irrelevant'' differences between structures such as white space or
permutations of items in a row of data set and hence expand the set of
transformations our system can define significantly.  Finally, we explored the
synthesis of \emph{symmetric lenses}~\cite{symmetric-lenses} , again building upon the
bijective platform that we started with.  Symmetric lenses are able to
ignore arbitrary chunks of data in A when generating target data B,
and vice-versa.  For example, a serial number specific to one data set
may be ignored when we translate to a second.  Symmetric lenses
further expand the set of allowed transformations.

We measured the effectiveness of our algorithms on a set of 39
benchmarks drawn from the Augeas system~\cite{augeas},  system for
transforming and editing Linux configuration files, and
FlashFill~\cite{flashfill}, an extension to Microsoft Excel for
synthesizing columns of spreadsheets.  Past synthesis
tools, such as FlashFill~\cite{flashfill} were unable to translate
most Augeas file formats.  However, our synthesis toolkit was able to
generate transforms for all 39 of the Augeas file formats that we analyzed.

We have written
papers~\cite{bijective-synthesis,quotient-synthesis,symmetric-synthesis}
describing and evaluating our algorithms and produced open-source 
code~\cite{GitHub} that implements those algorithms.  In this report,
we summarize some of our key methods, results and findings, focusing
on the core algorithms for bijective synthesis and quotient synthesis.

\section{Methods, Assumptions and Procedures}

Our approach combines two strategies.  We use \textit{domain-specific programming
  language}, specifically, the Boomerang~\cite{boomerang, Matching10} language,
which is designed for writing bidirectional string transformations.  We also exploit
\textit{type- and example-directed program synthesis}, which uses enumerative
techniques to search a space of candidate solutions.  The search is constrained
by the type information about the program, as well as by example instances
provided by the user.

\subsection{Lenses and lens domain-specific programming languages}

A lens comprises two functions,
\emph{get} and \emph{put}.  The \emph{get} function translates
source data into the target format.  If the target data is updated, the
\emph{put} function translates this edited data back into the
source format.  
A benefit of lens-based languages is that they use a single term
to express both 
\emph{get} and \emph{put}.
Furthermore, well-typed lenses give rise to 
\emph{get} and \emph{put} functions 
guaranteed to satisfy desirable invertibility properties.

Lens-based languages are present in variety of tools and have found mainstream
industrial use.
Boomerang~\cite{boomerang, Matching10} lenses provide
guarantees on transformations between {\em ad hoc} string document formats.
Augeas~\cite{augeas}, a popular tool that reads Linux system configuration
files, uses the \emph{get} part of a lens to transform configuration
files into a canonical tree representation that users can edit
either manually or 
programmatically.  It uses the lens's \emph{put} to merge the edited
results back into the original string format.  Other lens-based languages and
tools include 
%
GRoundTram~\cite{Hidaka2011GRoundTramAI},
%
BiFluX~\cite{DBLP:conf/ppdp/PachecoZH14}, 
%
BiYacc~\cite{DBLP:conf/staf/ZhuK0SH15},
%
Brul~\cite{DBLP:conf/etaps/ZanLKH16},
%
bidirectional variants of 
relational algebra~\cite{BohannonPierceVaughan},
%
BiGUL \cite{DBLP:conf/pepm/KoZH16}, 
%
spreadsheet formulas~\cite{DBLP:conf/vl/MacedoPSC14},
graph query languages~\cite{DBLP:conf/icfp/HidakaHIKMN10},
and
XML transformation languages~\cite{DBLP:conf/pepm/LiuHT07}.


\subsection{Bijective lens synthesis}

As inputs, our bijective synthesis procedure takes regular expressions specifying the
source, $S$, and
target, $T$, formats, plus a collection of concrete examples of the desired 
transformation.  Format specifications are supplied as ordinary regular
expressions.
Because regular expressions are so widely understood, we anticipate such
inputs will be substantially easier for everyday programmers to work with
than the unfamiliar syntax of lenses.
Moreover, these format descriptions communicate a
great deal of information to the synthesis system.  Thus, requiring user input
of regular expressions makes synthesis robust, 
helps the system scale to large and complex data sources, and 
constrains the search space sufficiently that the user typically needs
to give very few, if any, examples.

The goal of the synthesis algorithm is to find a lens
$\ell : S \Leftrightarrow T$ that corresponds to a bijection from the language
of a source regular expression $S$ to a target language of a target regular
expression $T$.   Such a lens should satisfy the \textit{bijective lens laws}:

\begin{equation}\label{bijectivelenslaws} \ell.\get \;
  (\ell.\lput \; t) = t \text{, and } \ell.\lput \; (\ell.\get \; s) = s
\end{equation}

\begin{figure}
  \centering
  \small 
  \begin{tikzpicture}[auto,node distance=1.5cm]
    \node[text width=1.5cm,minimum height=.6cm,align=center,draw,rectangle] (todnfregex) {\ToDNFRegex{}};
    
    \node[align=right, anchor=east] (regex1) [left = .6cm of todnfregex.north west]{\Regex{}};
    \node[align=right, anchor=east] (regex2) [left = .6cm of todnfregex.south west]{\RegexAlt{}};
    \node[align=right, anchor=east] (exs) [below = .2cm of regex2]{ \Examples{} };
    
    \node[align=center] (dnfregex1) [right = .4cm of todnfregex.north east]{\DNFRegex{}};
    \node[align=center] (dnfregex2) [right = .4cm of todnfregex.south east]{\DNFRegexAlt{}};

    \node[text width=3.1cm,minimum height=.6cm,align=center,draw,rectangle] [right = 1.45cm of todnfregex.east] (synthdnflens) {\SynthDNFLens{}};
    \node[align=center] [above = .7cm of synthdnflens] (optician) {\Optician{}};
    
    \node[align=center] [right = .4cm of synthdnflens] (dnflens) {\DNFLens{}};
    
    \node[text width=1.5cm,minimum height=.6cm,align=center,draw,rectangle] [right = .4cm of dnflens] (tolens) {\ToLens{}};
    
    \node[align=center] [right = .6cm of tolens] (lens) {\Lens{}};
    
    
    \path[->] (regex1.east) edge (todnfregex.north west);
    \path[->] (regex2.east) edge (todnfregex.south west);
    
    \path[->] (todnfregex.north east) edge (dnfregex1.west);
    \path[->] (todnfregex.south east) edge (dnfregex2.west);
    
    \path[->] (dnfregex1.east) edge (synthdnflens.north west);
    \path[->] (dnfregex2.east) edge (synthdnflens.south west);
    
    \path[->] (synthdnflens) edge (dnflens);
    
    \path[->] (dnflens) edge (tolens);
    
    \path[->] (tolens) edge (lens);
    \draw[->] ($(exs.east)+(-3pt,0)$) -| node(exsedge) {} (synthdnflens);
    \node[fit={($(todnfregex.west)+(-4pt,0)$) ($(tolens.east)+(4pt,0)$) (exsedge) (dnfregex1) (optician) (dnfregex2)},draw] (surrounding) {};
    % Now place a relation (ID=rel1)
    %\node[text width=2cm,align=center,draw, rectangle] (sketch-gen) [right = .75cm of spec] {\TypeProp{}};
    %\node (below-gen) [below=.5cm of sketch-gen] {};
    %\node[text width=2cm,align=center,draw, rectangle] (sketch-compl)
    %     [right = .25cm of sketch-gen] {\RigidSynth{}};
    %\node (below-compl) [below=.5cm of sketch-compl] {};
    %\node[align=center] (lens) [right = .75cm of sketch-compl] {Lens}; 
    %% Draw an edge between rel1 and node1; rel1 and node2
    %\path[->] (spec) edge node (start-alg) {} (sketch-gen);
    %\path[->] (sketch-gen) edge node(middle) {} (sketch-compl);
    %\path[->] (sketch-compl) edge node[near start](success) {\Success{}} (lens);
    %\draw[<-] (sketch-gen.south) -- +(0,-.5) -| node[above left](failure){\Failure{}} (sketch-compl.south);

    %\node (synth-name) [above=.5cm of middle] {\Optician{}};
    %
    %\node[fit=(sketch-gen) (sketch-compl) (start-alg) (synth-name) (failure)
    %(success) ,draw] (surrounding) {};

  \end{tikzpicture}
  \caption{Schematic Diagram for \Optician{}.  Regular expressions, \Regex{} and
    \RegexAlt{}, and examples, \Examples{}, are given as input.
    First, the function \ToDNFRegex{} converts \Regex{} and \RegexAlt{} into
    their respective DNF forms, \DNFRegex{} and \DNFRegexAlt{}.
    Next, \SynthDNFLens{} synthesizes a DNF lens, \DNFLens{}, from \Regex{},
    \RegexAlt{}, and \Examples{}.
    Finally, \ToLens{} converts \DNFLens{} into \Lens{}, a lens in Boomerang
    that is equivalent to \DNFLens{}.}
  \label{fig:schematic-diagram-synthesis}
\end{figure}

\Cref{fig:schematic-diagram-synthesis} shows a high-level,
schematic diagram for \Optician{}.
First, \Optician{} uses the function \ToDNFRegex{} to convert the input
regular expressions into DNF regular expressions.  Next,
\SynthDNFLens{} performs type-directed synthesis on these DNF regular
expressions and the input examples to synthesize a DNF lens.  Finally, this DNF lens is
converted back into a regular lens with the function \ToLens{}, and returned to the user.


\subsection{Quotient lens synthesis}

Quotient lenses are lenses in which the lens laws are
loosened so that they hold modulo an equivalence relation on the source and
target data respectively; as above, we are concerned with {\em bijective
quotient lenses} which are lenses for which Equation \ref{bijectivelenslaws}
holds modulo equivalence relations $\equiv_S$ and $\equiv_T$ defined on the source
and target data respectively:

\begin{equation}\label{quotientlenslaws}
\ell.\get \; (\ell.\lput \; t) \equiv_T t \text{, and } \ell.\lput \; (\ell.\get
\; s) \equiv_S s
\end{equation}

The inputs to the quotient lens synthesis problem, as in the bijective case,
include source and target regular expressions $S$ and $T$ and a set of example
instances.  The equivalence relations $\equiv_S$ and $\equiv_T$ are also given
as inputs to the synthesis algorithm.  Definition of such equivalences
are given via a new language of \textit{quotient regular expressions}
that we have defined.  Quotient regular expressions
describe both  $S$ (or $T$) and $\equiv_S$ (or $\equiv_T$)
simulataneously in one compact notation.

\subsection{Simple symmetric lens synthesis}
Although it is very useful to be able to synthesize bijective or
quotient lenses between data formats, such lenses do not fully solve the
problem. 
Bijective and quotient lenses require the two data formats to have
either precisely or morally the same information content. 
Many related data formats in practice have overlapping
information: some information may be present in one format but not the
other and vice versa.  

Symmetric lenses~\cite{HofmannPierceWagner10:POPL} address this
limitation.  General symmetric lenses introduce the notion of 
\textit{complement} that stores the information necessary to fully
reconstruct one format from the other.  Because such complements would
complicate the synthesis specificaiton problem, we instead define a
more restricted language of \textit{simple symmetric lenses}, which
are the largest class of symmetric lenses that do not rely on
persistent internal state to synchronise between two related data
formats, a property called \textit{forgetfulness}.  (Instead of a
complement, simple symmetric lenses rely on defaults to replace
missing information.)

A challenge in adopting the type-directed synthesis approach to simple
symmetric lenses is the number of such lenses.  Whereas the number of
bijective lenses between two formats is typically tiny, the
number of simple symmetric lenses is typically enormous. 
If a na\"ive  search algorithm just selects the first simple symmetric
lens it finds, the returned lens will generally not be the one the
user wanted. Symmetric synthesis requires a new principle for
identifying “more likely” lenses and a more sophisticated synthesis
algorithm that uses this principle to search the space more
intelligently.

For these, we turn to information theory. We consider ``likely'' lenses to
be ones that propagate ``a lot'' of information 
from the left data format to the right and vice versa. Conversely, ``unlikely''
lenses are ones that require a large amount of additional information to recover
one of the formats given the other. By default, our synthesis algorithm prefers
lenses that propagate more information.
This preference is formalized using \emph{stochastic regular expressions}
(SREs)~\cite{stoch-rnn,stoch-def}, which simultaneously define a set of
strings and a probability distribution over those strings.  Using this
probability distribution, we can calculate the likelihood of a given
lens.
We also allow users to override the default mechanism for
calculating the information content of a SRE by
asserting that certain strings are \emph{essential} or \emph{irrelevant},
forcing certain data to either be retained or discarded during the
transformations.



\begin{figure}
 \centering
  \includegraphics[width=.63\textwidth]{high-level-algorithm.pdf}
  \vspace{-2ex}
  \caption{Schematic diagram for the simple symmetric lens synthesis algorithm.
    The user provides regular expressions \BRegex and \BRegexAlt and a set of
    examples \Examples{} as input. \Expand first converts \BRegex and \BRegexAlt
    to stochastic regular expressions \Regex and \RegexAlt with default
    probabilities. It then finds pairs of stochastic regular expressions
    equivalent to \Regex and \RegexAlt and iteratively proposes them to
    \GreedySynth. \GreedySynth finds a lens typed between the supplied SREs.
    When the algorithm finds a likely lens, it returns it.}
  \label{fig:high-level-algorithm}
\end{figure}

With simple symmetric lenses and this SRE-based likelihood measure in hand, we
propose a new algorithm for synthesizing likely lenses. At its core, the
algorithm performs a type-directed search between descriptions of the data
formats, measuring success using the likelihood measure.

Interesting complications arise from the need to deal with regular-expression
equivalences. There are infinitely many regular expressions equivalent to a
given one, and the lens returned by a type-directed search will in general
depend on which of the possible representations are chosen for its source and
target formats. Moreover, certain lenses may not be well-typed unless the
format representations are replaced by equivalent ones:
in general, the synthesis algorithm has to search through
equivalent regular expression types to find the most likely lens. To tame
this complexity, we divide the synthesis algorithm into two communicating search
procedures (Figure~\ref{fig:high-level-algorithm}), following~\cite{optician}.
The first, \Expand, uses rewriting rules to propose new pairs of
stochastic regular expressions equivalent to the original pair. The second,
\GreedySynth, uses a greedy, type-directed
algorithm to find a simple symmetric lens between input SRE pairs, returning the lens and
its likelihood score to \Expand. The whole synthesis algorithm heuristically
terminates when a sufficiently likely lens is found.



\section{Results and Discussion}

% From bijective lens paper


% \newcommand{\SOptician}{Optician\textsubscript{S}}
% \newcommand{\SynthSymLens}{\PCF{SynthSymLens}\xspace}
% \newcommand{\RXSearchState}{\ensuremath{\mathit{pq}}}
% \newcommand{\ToStochastic}{\PCF{ToStochastic}\xspace}
% \newcommand{\LC}{\ensuremath{\mathit{lc}}}
% \newcommand{\Best}{\ensuremath{\mathit{best}}}
% \newcommand{\RXSearch}{\PCF{RXSearch}\xspace}
% \newcommand{\Continue}{\PCF{Continue}\xspace}
% \newcommand{\PQ}{\PCF{PQ}\xspace}
% \newcommand{\SynthDNFLens}{\PCF{SynthDNFLens}}
% \newcommand{\ToLens}{\ensuremath{\Uparrow}}
% \newcommand{\ToLensOf}[1]{\ensuremath{\ToLens{}\mkern-4mu #1}}
% \newcommand{\ToDNFRegexText}{\PCF{ToDNFRegex}}
% \newcommand{\Beautify}{\PCF{Beautify}}
% \newcommand{\RigidSynth}{\PCF{RigidSynth}}
% \newcommand{\GreedySynth}{\PCF{GreedySynth}\xspace}
% \newcommand{\RigidSynthInternal}{\PCF{RigidSynthInternal}}
% \newcommand{\RigidSynthSequence}{\PCF{RigidSynthSeq}}
% \newcommand{\RigidSynthAtom}{\PCF{RigidSynthAtom}}
% \newcommand{\GetDNFNormalizer}{\PCF{GetDNFNormalizer}}
% \newcommand{\CreatePQueue}{\PCF{CreatePQueue}}
% \newcommand{\GetTransitiveSet}{\PCF{GetTransitiveSet}}
% \newcommand{\GetCurrentSet}{\PCF{GetCurrentSet}}
% \newcommand{\Pop}{\PCF{Pop}}
% \newcommand{\ExpandOnce}{\PCF{ExpandOnce}}
% \newcommand{\ExpandRequired}{\PCF{ExpandRequired}}
% \newcommand{\FixProblemElts}{\PCF{FixProblemElts}}
% \newcommand{\Expand}{\PCF{Expand}\xspace}
% \newcommand{\ForceExpand}{\PCF{ForceExpand}}
% \newcommand{\Reveal}{\PCF{Reveal}}
% \newcommand{\Map}{\PCF{Map}}
% \newcommand{\EnqueueMany}{\PCF{EnqueueMany}}
% \newcommand{\ReturnVal}[1]{\ensuremath{\Return\,#1}}
% \newcommand{\CurrentSet}{\ensuremath{\mathit{CS}}}
% \newcommand{\TransitiveSet}{\ensuremath{\mathit{TS}}}

% \newcommand{\SSOpt}{\ensuremath{\mathbf{SS}}}
% \newcommand{\SSNCOpt}{\ensuremath{\mathbf{SSNC}}}
% \newcommand{\BSOpt}{\ensuremath{\mathbf{BS}}}
% \newcommand{\BSNCOpt}{\ensuremath{\mathbf{BSNC}}}
% \newcommand{\AnyOpt}{\ensuremath{\mathbf{Any}}}
% \newcommand{\FLOpt}{\ensuremath{\mathbf{FL}}}
% \newcommand{\CCOpt}{\ensuremath{\mathbf{DC}}}
% \newcommand{\NSOpt}{\textbf{NS}}
% \newcommand{\NROpt}{\textbf{NR}}

We have implemented our synthesis algorithms in
of OCaml and integrated them into Boomerang system,
where they are available as open source code~\cite{GitHub}.
Users of Boomerang can write lenses by hand, specify them as
synthesis tasks, or do a combination of both.

We have analyzed our algorithms theoretically, proving various
soundness and completeness results.  We have also analyzed our
algorithms empirically, studying their impact on a range of real and
synthetic benchmarks.  Our full results have been published in a
series of conference papers and technical
reports~\cite{bijective-synthesis,quotient-synthesis,symmetric-synthesis}.  In the rest of this
section, we summarize some of the key results.

\subsection{Bijective Synthesis Algorithms}

We first study the effectiveness of the pure bijective synthesis
algorithm by evaluating its performance on a set of 39
benchmark programs.  
%
All evaluations were performed on a 2.5 GHz Intel Core i7 processor with 16 GB
of 1600 MHz DDR3 running macOS Sierra.

\paragraph*{Benchmark Suite Construction}
We constructed our benchmarks primarily by adapting examples from
Augeas~\cite{augeas} and 
Flash Fill~\cite{gulwani-popl-2014}.
%
Augeas is a configuration editing system for Linux that uses lens
combinators similar to those in Boomerang. However, it transforms
strings on the left to structured trees on the right rather than
transforming strings to strings.
We adapted these Augeas lenses to our setting by converting the
right-hand sides to strings that correspond to serialized versions
of the tree formats.  
We derived 29 of the benchmark tests by
adapting the first 27 lenses in alphabetical order, as well as the lenses
\CF{aug/xml-firstlevel} and \CF{aug/xml} that were referenced
by the `A' lenses.
Furthermore, the 12 last synthesis problems derived
from Augeas were tested after \Optician{} was
finalized, demonstrating that the optimizations were not
overtuned to perform well on the testing data.
%
Flash Fill is a system that allows users to specify common string
transformations by example~\cite{gulwani-popl-2014}.  
We derived three benchmarks from the first few examples in the
paper and one from the running example on
extracting phone numbers.

Both Augeas and Flash Fill permit non-bijective transformations.
To test our system on these benchmarks, we had to convert them into
bijective transformations by hand.  More specifically, in many of the
benchmarks, we normalized
the whitespace in the documents so the amount of whitespace was equal
in the source and target.  In a few of the benchmarks, either the
source or the target was missing information present in the other
format.  We modified those benchmarks by adding the missing
information back into the file. 

Finally, we added custom examples to highlight weaknesses of
our algorithm (\CF{cap-prob} and \CF{2-cap-prob}) 
and to test situations for which we thought the tool would be
particularly useful (\CF{workitem-probs}, \CF{date-probs}, \CF{bib-prob},
and \CF{addr-probs}).   These examples convert between work item formats, date
formats, bibliography formats, and address formats, respectively.

\begin{figure}
  \centering
  \begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics{figs/specsizes}
    \caption{}
    \label{subfig:lenssize}
  \end{subfigure}
  \begin{subfigure}[b]{.49\textwidth}
    \includegraphics{figs/examplesused}
    \caption{}
    \label{subfig:examplesused}
  \end{subfigure}
  \caption{Sizes of Specifications.
    In (a), we show how many benchmarks are defined in our suite using a
    given number of AST nodes or fewer.
    In (b), we show how many benchmarks are defined in our suite using a
    given number of examples or fewer.}
  \label{fig:definition-sizes}
\end{figure}

Figure~\ref{fig:definition-sizes} shows the complexity of our type-based
specifications as well as our example counts.
An average benchmark has a type-based specification that can be represented using
310 AST node, and requires 1.1 input/output examples.
Our benchmarks vary from simple problems, like changing the
representation of dates
(with a specification size of 85, and a generated lens size of 79), to 
complex tasks, like transforming configuration files for server monitoring
software into dictionary form (with a specification size of 670 and
a generated lens size of 651).  On average, the size of the generated lens is
89\% the size of its type specifications.


\paragraph*{Importance of Examples}

\begin{figure}
  \centering
  \includegraphics{figs/examples.eps}
  \caption{Average number of random examples required to synthesize benchmark
    programs.  {\bf Experimental Average} is the average number of randomly
    generated examples needed to correctly synthesize the lens.  {\bf
      Determinize Permutations} is the theoretical number of examples required
    to determinize the choice all the permutations in \RigidSynth{}.
    In practice, far fewer examples are
    needed to synthesize the correct lens than would be predicted by the number
    required to determinize permutations.}
  \label{fig:exs-reqd}
\end{figure}

To evaluate how many user-supplied examples the algorithm requires in
practice, we \textit{randomly} generated appropriate source/target
pairs, mimicking what a na\"{i}ve user might do.  We did not write the
examples by hand out of concern that our knowledge of the synthesis
algorithm might bias the selection. Figure~\ref{fig:exs-reqd} shows
the number of randomly generated examples it takes to synthesize the
correct lens averaged over ten runs.  The synthesis algorithm almost never needs
any examples: only 5 benchmarks need a nonzero number of examples to
synthesize the correct lens and only one, \CF{cust/workitem-probs} required over
10 randomly generated examples.
A clever user may be able to reduce the
number of examples further by selecting examples carefully; we
synthesized \CF{cust/workitem-probs} with only 8 examples.

These numbers are low because there are relatively few well-typed
bijective lenses between any two source and target regular expressions. 
As one would expect, the benchmarks where there are multiple ways to
map source data to the target (and vice versa) require the most examples.
For example, the benchmark \CF{cust/workitem-probs} requires a large number of
examples because it
must differentiate between data in different text fields in both the
source and target and map between them appropriately.  As these text fields are
heavily permuted
(the legacy format ordered fields by a numeric ID, where
the modern format ordered fields alphabetically) and fields can be
omitted, a number of examples are needed to correctly identify the mapping
between fields.

The average number of examples to
infer the correct lens does not tell the whole story.  The system will
stop as soon as it finds a well typed lens that satisfies the supplied examples.
This inferred lens may or may not 
correctly handle unseen examples that correspond to
unexercised portions of the source and target regular expressions.
Figure~\ref{fig:exs-reqd} lists
the number of examples that are required to determinize the generation of
permutations in \RigidSynth{}.
Intuitively, this number represents the maximum number of
examples that a user must supply to guide the synthesis engine if it
always guesses the wrong permutation when multiple permutations can be used to
satisfy the specification. 

The average number of examples is so much lower than the maximum
number of required examples because of correspondences in how we wrote
the regular expressions for the source and target data formats. 
Specifically, when we had corresponding disjunctions in both the
source and the target, we ordered them the same way.  The algorithm
uses the supplied ordering to guide its search, and so the system
requires fewer examples.   We did not write the examples in this style
to facilitate synthesis, but rather because maintaining similar
subparts in similar orderings makes the types much easier to 
read. We expect that most users would do the same.

\paragraph*{Comparison Against Other Tools}
%
We are the first tool to synthesize bidirectional transformations between data
formats, so there is no tool to which we can make an apple-to-apples comparison.
Instead, we compare against tools for generating unidirectional
transformations instead. 
Figure~\ref{fig:synthesis-times} includes a comparison against two other
well-known tools that synthesize
text transformation and extraction functions from examples -- Flash Fill and FlashExtract.  For this
evaluation, we used the version of these tools distributed through the
PROSE project~\cite{prose}.

\begin{figure}
  \includegraphics{figs/times}
  \caption{
    Number of benchmarks that can be solved by a given algorithm in a given
    amount of time.
    \Optician{} is our bijective synthesis algorithm including a set
    of optimizations we have implemented.
    \FlashExtractMode{} is the existing FlashExtract system.  \FlashFillMode{} is
    the existing Flash Fill system.  \NaiveMode{} is na\"{i}ve type-directed 
    synthesis on the bijective lens combinators.  Our synthesis algorithm performs
    better than the na\"{i}ve approach and other string transformation systems,
    and our optimizations speed up the algorithm enough that all tasks become
    solvable.
  }
  \label{fig:synthesis-times}
\end{figure}

To generate specifications for Flash Fill, we generated input/output
specifications by generating random elements of the source language, and
running the lens on those elements to generate elements of the target language.
These were then fed to Flash Fill.

To generate specifications for FlashExtract, we extracted portions of strings
mapped in the generated lens either through an identity
transformation or through a previously synthesized lens, whereas strings that were
mapped through use of \ConstLens{} were considered boilerplate and so not
extracted.

As these tools were designed for a broader audience, they put less of a burden
on the user.  These tools only use input/output examples (for Flash
Fill), or marked text regions (for FlashExtract), as opposed
to \Optician{}'s use of regular expressions to constrain the format of
the input and output.  By using regular expressions,
\Optician{} is able to synthesize significantly more programs
than either existing tool.

Flash Fill and FlashExtract have two tasks: to determine how
the data is transformed, they must also infer the structure of the data, a
difficult job for complex formats.
In particular, neither Flash Fill nor FlashExtract was able to synthesize
transformations or extractions present under two iterations, a type of format
that is notoriously hard to infer.
These types of dual iterations are pervasive in Linux configuration
files, making Flash Fill and FlashExtract ill suited for many of the synthesis
tasks present in our test suite.

Furthermore, as unidirectional transformations, Flash Fill and FlashExtract have
a more expressive calculus.  To guarantee bidirectionality, our syntax must be
highly restrictive, providing a smaller search space to traverse.

\subsection{Quotient Synthesis Algorithms}

% DPW:  From Quotient lenses paper

\newcommand{\wf}[1]{\ensuremath{#1\;\mathsf{wf}}}

% FOR Regular Expression names
\newcommand{\re}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\codefont}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\kw}[1]{\textcolor{dkblue}{\ensuremath{\mathsf{#1}}}}
\newcommand{\collapse}[2]{\ensuremath{\kw{collapse} \; #1 \mapsto #2}}
\newcommand{\squash}[3]{\ensuremath{\kw{squash} \; #1 \rightarrow #2\; \kw{using} \; #3}}
\newcommand{\perm}[2]{\ensuremath{\kw{perm}(#1)\; \kw{with}\; #2}}
\newcommand{\normalize}[3]{\ensuremath{\kw{normalize}(#1, #2, #3)}}
\newcommand{\eqrel}[1]{\ensuremath{\equiv_{#1}}}

\newcommand{\canonize}{\ensuremath{\kw{canonize}}}

\newcommand{\Name}{Optometrist\xspace}

\newcommand{\QRESize}{\textbf{QS}}
\newcommand{\canonizeAndSpecSize}{\textbf{BS}}
\newcommand{\LensAndSpecSize}{\textbf{NS}}

%\newcommand{\QOpt}{Optician_Q}
\newcommand{\QOpt}{QRE-enhanced Optician}
\newcommand{\OpticianRuntime}{\textbf{Optician}}
\newcommand{\QREOptician}{\textbf{Optician\textsubscript{Q}}}
\newcommand{\SystemOnOptician}{\textbf{QO}}
\newcommand{\SystemOnBenchmarks}{\textbf{QQ}}
\newcommand{\cd}[1]{\lstinline[backgroundcolor=\color{white}]$#1$}

We have extended the bijective synthesis tool to synthesize quotient lenses. We will use 
``\QOpt'' to denote our extended version of Optician, and just plain ``Optician'' to denote
the pre-quotient version of Optician. The synthesis algorithm produces Boomerang lens
values, so Boomerang gives synthesized lenses the same first-class status as hand-written ones. 
%
All evaluations were performed on a 2.5 GHz Intel Core i7 processor with 16 GB
of 1600 MHz DDR3 running macOS High Sierra.


\paragraph*{Benchmark Suite Construction}

We analyzed the same 39 lens synthesis tasks from the original
Optician system.
We also experimented using our tool to synthesize quotient lenses
between XML, RDF and and JSON formats using data from the data.gov
database; the data consisted of census statistics, demographic
statistics, wage comparosion data, and crime index data).
Recall that to use bijective synthesis,  we had to modify 10 of 39
benchmarks make them bijective (for instance, by normalizing whitespace)
Because quotient lenses are more expressive, we were able avoid such
modifications. This
experience alone points to the benefits that quotient lens synthesis
brings to the table. 

\paragraph*{Evaluating programmer effort}

\begin{figure}[t]
\includegraphics{qfigs/asts.eps}
\caption{AST node measurements for each of the three approaches
on each of the 10 non-bijective benchmark problems.
Benchmarks are sorted in order of increasing complexity as measured by
the number of AST nodes in the source and target format descriptions. 
QRE Synthesis requires far fewer AST nodes than the other
two approaches.}
%%\caption{Count of benchmark programs definable using a given AST count. We find
%%that it takes far fewer AST nodes to define benchmark lenses using QRE
%%synthesis than with Optician without QRE synthesis or without synthesis.}
\label{fig:asts}
\end{figure}

To evaluate the impact of QRE lens synthsis on programmer effort,
we focus our attention on the 10 problems in the benchmark suite that
are not bijective and hence require non-trivial canonizers. 
(Optician already handles the other problems with minimal programmer
effort.)

We are interested in comparing three different approaches, which vary
in the amount of synthesis used. 
In the first approach, which we call \QRESize{}, the programmer uses
quotient lens synthesis.  She must write QRE specifications of the source and
target formats and she may give examples.
In the second approach, which we call \canonizeAndSpecSize{} for
Bijective Synthesis, the
programmer uses bijective lens synthesis \`a la Optician.
She must write canonizers by hand, along with 
regular expressions to describe the external
representations of the source and target formats. (The internal
formats can be inferred from the canonizers.) She may also
provide examples to help in the synthesis of the bijective lens.
In the third approach, which we call \LensAndSpecSize{} for No Synthesis, the
programmer writes the lens between the source and target formats
entirely by hand, including the descriptions of the source and target
formats.

For each problem in the benchmark suite, we calculate the following
measures as proxies for the level of programmer effort when using each
the three approaches:

%
\begin{itemize}
  \item[\QRESize{}:] 
  The number of AST nodes in the QRE specifications for the source and
  target formats, including examples. 
  \item[\canonizeAndSpecSize{}:] 
  The sum of (1) the number of AST nodes in $W(q)$ for each QRE $q$ in the source and target
  formats, (2) the number of AST nodes in $\canonize(q)$ for each QRE $q$ with a
  non-trivial canonizer, and (3) the number of AST nodes in the
  examples.  We use (1) to estimate the burden of describing
  the external source and target formats and (2) to estimate the
  burden of writing the requisite canonizers
  by hand.  We count the nodes in the examples because they would be
  fed to the bijective synthesizer.  
  These counts are an approximation, as both $W(q)$ and $\canonize(q)$ are
  automatically generated from the corresponding QRE $q$, and it is
  possible that a human-written version might be smaller.
  \item[\LensAndSpecSize{}:] The sum of (1) the number of AST nodes in
  $W(q)$ for each QRE $q$ in the source and target formats and (2) the
  number of AST nodes in the synthesized QRE lens.  We use (1) to
  estimate the burdern of describing the source and target formats
  and (2) to estimate the burdern of writing the appropriate lens by
  hand. These counts are also approximations, as
  $W(q)$ and the synthesized lens may be larger than one written by hand.
\end{itemize}

Figure~\ref{fig:asts} shows each of these measures for the 10
non-bijective problems in the benchmark suite.  On
average (using a geometric mean), \canonizeAndSpecSize{} used~38.5\% more AST nodes
than \QRESize{}, requiring an average of~214 more AST nodes. On 
average, \LensAndSpecSize{} used~180\% more AST nodes than \QRESize{}, requiring an
average of~998 more AST nodes. These figures suggest that introducing QREs saves
programmers significant effort compared to both Optician and basic
Boomerang.

\paragraph*{Quotient Lens Performance}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.49\textwidth}
\centering
\includegraphics{qfigs/times_opt}
\caption{}
\label{subfig:lenssize}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\includegraphics{qfigs/times_new.eps}
\caption{}
\label{subfig:examplesused}
\end{subfigure}
\caption{Runtimes measurements. In (a), we run Optician and \QOpt{}
  on the Optician benchmarks.  We find that there is only a negligible
  performance overhead incurred by using QREs. 
  In (b), we run \QOpt{} on the 10 Optician benchmarks
  previously edited to make them bijective, after removing those edits
  and then extending the synthesis specification to include QREs.
  (In other words, we restored them to their original state, added QREs,
  and then ran \QOpt{}). 
  We find that \QOpt{} is able to
  synthesize all quotient lenses in under 10 seconds, and typically finishes in
  under 5 seconds.}
\label{fig:times}
\end{figure}

To assess the performance of quotient synthesis, we are interested in two different
questions. First, how does the performance of \QOpt{} compare to the performance
of Optician on benchmarks that do not require quotients? The answer to this question
tells us how much overhead we have introduced by adopting the more general
mechanism. Figure~\ref{fig:times}(a) shows that \QOpt{} was able to synthesize
all of the Optician benchmarks at a speed competitive with the old version.
There is a small amount of additional overhead introduced by QREs in calculating
equivalences, resulting in a slight decrease in performance.

Second, how much time does it take for \QOpt{} to synthesize a quotient
lens when running on a non-bijective benchmark problem?  
Figure~\ref{fig:times}(b) shows the amount of time required to infer a
lens for each of the 10 benchmark programs with nontrivial quotients.  
We find that \QOpt{} is able to synthesize all quotient lenses in
under 10~seconds, and typically finishes in under 5~seconds.


\section{Conclusions}

During the course of this project, we have designed, analyzed and
implemented algorithms that demonstrate it is possible to synthesize
three classes of bidirectional transformations: (1) pure bijective
transformations, (2) bijections modulo equivalences classes (quotient
lenses) and (3)
bijections modulo projections (symmetric lenses).  The synthesis algorithms we have
designed take inputs that include a format specification (which
includes specification of equivalence classes) and a collection of
examples.  As the class of transformations becomes richer, the number
of potential programs grows dramatically.  As a result, the
corresponding inference algorithm slows and its ability to guess the
transformation desired by the user decreases, leading to reduced
accuracy.  We found that it was possible to overcome such challenges
through new heuristics that use information theory to help guide the
search for program transformations.  In addition, we found the use of
compositional synthesis, which is the process of breaking down a
complex synthesis task into smaller, more mangeable subtasks critical
when attempting to scale our prototype system up to be able to handle
larger transformation task.

\bibliographystyle{apalike}
\bibliography{local.bib,bcp.bib}

\section{List of Symbols, Abbreviations, and Acronyms}
\begin{tabular}{ll}
  DARPA & Defense Advanced Research Project Agency \\
\end{tabular}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
