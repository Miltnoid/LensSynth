Thank you for your detailed feedback.  We will incorporate your many
useful suggestions when we revise our paper. 

Several reviewers asked who benefits from this technology.   The
brief answer is that this technology is aimed at programmers.  If one
were to ask random programmers whether they know how to use regular
expressions, most would say yes.  In contrast, almost none of them
would know how to use lenses. The central idea of this paper is to
exploit something that most programmers know well (regular exprssions)
to help them write complex programs in a language that they do not
know well (lenses).  This help is particularly important because
lenses are *tricky* to write.  For example, consider the following
lens for translating between idealized bibtex and endnote author
formats:

let bibtex_to_endnote_au
  : (lens in BIBTEXAUTHORINFO <=> TAGGEDAUTHORDEFNS) =
  del "author={"
    . ins "au - "
    . lens_swap (NAME . del ",") (lens_swap WSP NAME)* 
    . (del " and "
         . ins "\n au - "
         . lens_swap (NAME . del ",") (lens_swap WSP NAME)* )* 
    . del "}" 

Writing this lens from scratch is difficult, and it is a relatively
simple lens! We will rewrite the introduction to clarify the goals of
our work.

A second concern stemmed from the lack of a formal treatment of our
implementation, in particular the function RigidSynth. In earlier
drafts, we had a formal description of this algorithm.  Regrettably,
we removed it to shorten the paper to fit the page limit.  We
inadvertently removed it from the extended report at the same time.
We will reinstate the full formal description in the extended report
and find ways to summarize it precisely in the final paper.

A third area of concern involved aspects of our evaluation,
specifically comparisons with other tools and measurements to evaluate
the heuristics we chose in searching for lenses.

A direct comparison with related tools is not possible because there
are no other tools that do bidirectional program synthesis.
Performance comparisons to similar tools such as FlashFill and
FlashExtract  are difficult because they cannot express the rich set
of permutations we tackle.  

As for our choice of heuristics, we arrived at them when more naive
approaches failed.  For example, our initial search procedure, which
traversed arbitrary equivalences, could not synthesize anything beyond
simple microbenchmarks, even with the significant simplification of
not searching for potential compositions!  Without a priority queue,
we were able to synthesize complicated lenses when the source and
target formats were similar, but not when the formats differed
significantly. We agree that performance measurements showing the
importance of these aspects of our algorithm would improve the paper,
and we will work to add them.

In the remainder, we respond to individual concerns not already addressed.

==========================
Review A:
==========================

- The synthesized lenses are not shown or discussed in much detail,
  nor are they compared to hand-written ones (when possible).

The synthesized lenses are fairly comprehensible.  The transformation from DNF
lenses to Boomerang lenses alone creates fairly unintelligible syntax.  This is
an issue, we would like for programmers to be able to validate the synthesized
programs through a manual inspection of them.  As such we apply a number of
optimizations, including, but not limited to:

1. Trivial sublenses are removed
2. Sublenses expressing identity transformations are turned into identities
3. Identity transformations are combined when possible
4. Distributed transformations are factored when possible

The previously shown lens, bibtex_to_endnote_au, is an autogenerated lens.
The primary difference we see between the autogenerated code, and what we would
write ourselves, is the repeated code of

ins "au - "
. lens_swap (NAME . del ",") (lens_swap WSP NAME)*

We personally do not find this to make the code significantly more difficult to
read.  However, adding this optimization is merely a rote application of common
subexpression elimination.


- The comparison and relationship to Flash Fill, clarified in the
  evaluation, is too broad in the motivating sections.

Addressed above


- The prevalence and limitations of bijective lenses (compared to
  less well-behaved ones) is not discussed.

There certainly are limitations to restricting ourselves to bijective lenses.
However, we chose the simplification of bijective lenses for this new research
area, but think there is a wealth of future work that can be done through an
investigation of less well-behaved lenses.

- How many of surveyed FlashFill examples were close to bijective? Were there
  only the 3 included in the evaluation?

Few of the surveyed FlashFill examples were close to bijective.  The tricks we
used to make our chosen 3 examples bijective - adding in extra fields to hold
bits of data - could be extended to non-bijective examples.  Our primary goal
with the use of FlashFill was to show that, when made bijective and given data
format descriptors, the problems FlashFill addresses become simple.


==========================
Review B:
==========================

- paper unclear in various (important) places

We unfortunately were had to be brief in our description of confluence,
bisimilarity, and definitional equivalence.  We go into much greater detail in
the appendix, and will make sure to refer to the appendix for a more detailed
description of these terms, and for a more formal understanding of their
importance.


- algorithm only partially presented

Addressed above


- I am not sure the remark about the implementation spanning '5515 lines' is as
positive as you make it sound at the beginning of Section 6. There is no
pseudocode for the main part of the algorithm, a function called RigidSynth.
This makes one wonder how complex it really is and, more worryingly, as a
result, there is also no correctness proof and no complexity analysis.

Addressed in part above.  Many of the lines go not to the core synthesis
algorithm, but for language transformations, pretty printing, lens
simplification, and DFA operations.  The portion of our code corresponding to
RigidSynth is just over 400 lines.


- some proofs are missing

Please refer to our appendix, where we go into a detailed proof of these
theorems.

- It would be nicer if runtimes were compared with other programs. For example,
  the related work section mentions FlashFill as 'one of the most prominent
  recent string transformation systems', so since part of the benchmarks are
  adapted from FlashFill benchmarks, it would have been interesting to compare
  the performance here. The authors also say that 'there are many other recent
  results showing how to synthesize functions from type-based specifications'
  and that they 'found we needed new techniques to manage the multidimensional
  search space generated by regular expression equivalences and other aspects of
  lens combinator languages'. I am missing either an experimental comparison
  with these other results or a more detailed explanation of why they would not
  be applicable to their problem.

Addressed above





==========================
Review C:
==========================

- The experimental section could be stronger. I understand that comparisons with
  other tools is not feasible in this setting, as there aren't any other tools
  that are solving this specific variety of synthesis. However, you could do
  some internal comparisons, for various values of the design decisions.

Discussed above


- The discussion of related work can be improved. It is true that there aren't
  other approaches that are solving the same exact problem. However, some
  aspects of your synthesis are shared with other approaches.

Agreed, while we use similar techniques, they are used in different ways,
identifying the similarities and noting the differences would strengthen our
related work.





==========================
Review D:
==========================

- The algorithmic contributions (particularly, type-directedness) are often
  unclear.

Discussed in part above.
RigidSynth is entirely type directed.  We use an ordering on the subcomponents
of the types to find which subcomponents must be mapped to each other.  After
these lenses are found, they are then combined into the final lens in a way
that keeps the lens well typed.

- Converting a regular expression to DNF may incur an exponential explosion (as
it is akin to determinizing an NFM). Have you encountered examples where that is
the case? Please state this shortcoming.

We have not encountered any examples in practice where this is a problem.
However, we include in our benchmarks cust/cap-prob and cust/2-cap-prob to
highlight this shortcoming.  Furthermore, we discuss the shortcoming in Speed of
Synthesis in the evaluation section.


- Have you considered using a SAT/SMT solver to search the space of
  permutations?

Because of our use of orderings, we are able to find the permutations in
O(n*log(n)) time.


- The experimental evaluation is limited and does not back the claimed
  contributions.  Similar to (a), what is the effect of type-directedness on
  the experimental results?

Discussed in part above.
We did not ever try an approach that was not type-directed, as we do not think
enumerative search followed by a pruning of ill typed terms would scale well.


- The fact that the avg # of examples is often 0 seems to indicate that the
  benchmarks are simple. This is perhaps because the language is very
  restricted; i.e., there are no user-defined lenses, only the core ones. Could
  you comment on that?

We discuss why the average number of examples is so low in Importance of
Examples in the evaluation section.  To highlight how complicated the input
specifications can be, the specification for the BibTex to
EndNote conversion problem - one of our 10 least complex input formats - and the
specification for cron - our most complex format - are given
below.  We are happy to provide all our specifications.

cust/bib_prob
      #use "base.decls"
      typedef NAME = UPPERCASE (LOWERCASE)*;;
      
      typedef LASTCOMMASTART = NAME "," (WSP NAME)*;;
      
      typedef STARTTOEND = (NAME WSP)* NAME;;
      
      typedef BIBTEXAUTHORLIST = LASTCOMMASTART (" and " LASTCOMMASTART)*;;
      typedef BIBTEXAUTHORINFO = "author={" BIBTEXAUTHORLIST "}";;
      
      typedef AUTAG = "au - ";;
      typedef TITAG = "ti - ";;
      typedef JOTAG = "jo - ";;
      
      typedef TAGGEDAUTHORDEFNS = AUTAG STARTTOEND ("\n " AUTAG STARTTOEND)*;;
      
      typedef TITLE = NAME (WSP NAME)*;;
      typedef BIBTEXTITLE = "title={" TITLE "}";;
      typedef TAGGEDTITLE = TITAG TITLE;;
      
      typedef JOURNAL = NAME (WSP NAME)*;;
      typedef BIBTEXJOURNAL = "journal={" JOURNAL "}";;
      typedef TAGGEDJOURNAL = JOTAG JOURNAL;;
      
      typedef FULLBIBTEX = "{" ((BIBTEXJOURNAL | BIBTEXAUTHORINFO | BIBTEXTITLE)",")* "}";;
      typedef FULLTAGS = . | ((TAGGEDAUTHORDEFNS | TAGGEDTITLE | TAGGEDJOURNAL)
      (("\n" (TAGGEDAUTHORDEFNS | TAGGEDTITLE | TAGGEDJOURNAL))*)) ;;
      
      bibtex_to_readable_au = [BIBTEXAUTHORINFO <=> TAGGEDAUTHORDEFNS
      {"author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron}" <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon"}]
      
      bibtext_to_readable_title = [BIBTEXTITLE <=> TAGGEDTITLE
      {"title={Boomerang Resourceful Lenses For String Data}" <->
       "ti - Boomerang Resourceful Lenses For String Data"}]
      
      journal_to_readable_journal = [BIBTEXJOURNAL <=> TAGGEDJOURNAL
      {"journal={Principals Of Programming Languages}" <->
       "jo - Principals Of Programming Languages"}]
      
      bibtext_to_tagged_tester= [FULLBIBTEX <=> FULLTAGS {
      "{author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron},title={Boomerang Resourceful Lenses For String Data},journal={Principals Of Programming Languages},}"
      <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon
      ti - Boomerang Resourceful Lenses For String Data
      jo - Principals Of Programming Languages"
      }]
      
      test bibtext_to_tagged_tester
      {"{author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron},}"
      <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon",
      
      "{title={Boomerang Resourceful Lenses For String Data},}"
      <->
      "ti - Boomerang Resourceful Lenses For String Data",
      
      "{journal={Principals Of Programming Languages},}"
      <->
      "jo - Principals Of Programming Languages"
      };;

aug/cron
      #use "base.decls"
      #use "util.decls"
      
      typedef INDENT = (" " | "\t")*;;
      typedef INDENT_REQ = (" " | "\t")+;;
      typedef ALPHANUM = (UPPERCASE | LOWERCASE | DIGIT)+;;
      typedef RANGE = (ALPHANUM "-" ALPHANUM | ALPHANUM );;
      typedef PREFIX = "-";;
      
      typedef SCHEDULE_VALUE = "reboot" | "yearly" | "annually" | "monthly"
                             | "weekly" | "daily" | "midnight" | "hourly";;
      typedef SCHEDULE = "@" SCHEDULE_VALUE;;
      
      typedef USER = (UPPERCASE | LOWERCASE | DIGIT)+;;
      
      typedef TIME = NUMBER INDENT_REQ NUMBER INDENT_REQ NUMBER INDENT_REQ RANGE INDENT_REQ RANGE;;
      
      typedef SHELLCOMMAND_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|"  | "." ;;
      typedef SC_CHAR_OR_SPACE = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|" | "." | " " ;;
      typedef SHELLCOMMAND = (SHELLCOMMAND_CHAR (SC_CHAR_OR_SPACE)* SHELLCOMMAND_CHAR) | SHELLCOMMAND_CHAR;;
      
      typedef SHELLVAR_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_";; 
      typedef SHELLVAR_NAME = SHELLVAR_CHAR+;;
      typedef SHELLVALUE_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|" | ".";;
      typedef SHELLVALUE_NAME = SHELLVALUE_CHAR+;;
      
      typedef SHELLVAR = SHELLVAR_NAME "=" SHELLVALUE_NAME "\n";;
      typedef COMMENTLINE = COMMENT "\n";;
      typedef ENTRY = INDENT (PREFIX | . ) (TIME | SCHEDULE) INDENT_REQ USER INDENT_REQ SHELLCOMMAND "\n";;
      typedef CRON = ( "\n" | SHELLVAR | COMMENTLINE | ENTRY)*;;
      
      typedef PREFIX_DICT = "{\"prefix\"=" ("true" | "false") "}";;
      typedef TIME_DICT = "{\"minute\"=" NUMBER ",\"ws1\"=" INDENT_REQ ",\"hour\"=" NUMBER 
        ",\"ws2\"=" INDENT_REQ ",\"dayofmonth\"=" NUMBER ",\"ws3\"=" INDENT_REQ 
        ",\"month\"=" RANGE ",\"ws4\"=" INDENT_REQ ",\"dayofweek\"=" RANGE "}";;
      typedef SCHEDULE_DICT = "{\"schedule\"=\"" SCHEDULE_VALUE "\"}";;
      typedef ENTRY_DICT = "{\"indent\"=\"" INDENT "\"," PREFIX_DICT "," (TIME_DICT | SCHEDULE_DICT)
        ",\"indent2\"=\"" INDENT_REQ "\",\"user\"=\"" USER "\",\"indent3\"=\""
        INDENT_REQ "\",\"command\"=\"" SHELLCOMMAND "\"}";;
      typedef SHELL_DICT = "{\"varname\"=\"" SHELLVAR_NAME "\",\"value\"=\"" SHELLVALUE_NAME "\"}";;
      typedef CRON_DICT = ((EMPTYDICT | SHELL_DICT | COMMENT_DICT | ENTRY_DICT) "\n")*;;
      
      cron_lens = [CRON_DICT <=> CRON {}]





==========================
Review E:
==========================

- The current presentation doesn’t explain what are the difficult challenges for
  this problem. Given the source and target regular expressions, and especially
  with the strongly unambiguous constraint, it seems a simple enumerative search
  should work well. I was wondering if the authors tried simpler enumerative
  solvers baselines for this problem?

Addressed above


- The second big concern is that it was not obvious who is the target audience
  for this system. If we are expecting a user to provide the regular expressions
  for source and target formats, why can’t they also write the transformation as
  well? Can the authors provide the Boomerang program for the Bibtex example and
  explain why it would be challenging for a user to write this Boomerang program
  if they can write regular expressions for the two formats.

Addressed above


- For the Bibtex to EndNote transformation example, the paper presents an
  algorithmic way to align the regular expressions to find corresponding
  permutations of sub-expressions to match different fields. Why can’t the user
  provide regular expressions in first place that are aligned? For example,
  there is an epsilon in the Bibtex regular expression, but there is no epsilon
  in the EndNote format. Wouldn’t it be a more clear and a better specification
  on user’s part to write a regular expression where EndNode format also has an
  epsilon with an Or in front. For finding permutations as well, it isn’t clear
  why can’t the user simply annotate or label different fields using some user
  interface, rather than the algorithm having to search over different matching
  of sub-expressions.

We feel that requiring well aligned regular expressions is too large a burden on
the user.  The user themselves needs to think through various equivalence rules,
which we think is better suited to an automated tool.

The idea of adding in the ability to annotate fields is an interesting one.
The way that examples are embedded in the regular expressions is similar to an
annotation.  In certain scenarios, it may also be easier to give annotations
than to give a covering example.  Likely the best solution would be one which
uses both annotations and examples to help differentiate similar fields.


- Is there a restriction that the user-provided regular expressions need to
  parse the source and target formats unambiguously? For example, if a user
  provides the regular expression for a format as Name* Name *, and the input is
  John Conway, would the system disallow such user inputs?

There is such a restriction (though we allow for ambiguity when hidden under
user defined regular expressions), but the tool does not currently check that
this restriction is held.  Methods for preforming these checks are well studied,
and described in Boomerang as well as in Nate Foster's thesis.  An end goal for
this tool is to be integrated with the Boomerang codebase, and used as part of
the development for Boomerang programs.  Such a production quality version of
this tool would certainly provide these capabilities.


- The synthesis algorithm section presents several heuristics to assign scores
  and pseudometric scores to a pair of DNF regular expressions. How were these
  heuristics chosen? What is the effect of these heuristics on the benchmark
  problems?

Discussed in part above.  These heuristics were chosen in a way to leverage
the abstractions that programmers use for their specifications, while also
allowing enough breadth to the search space to not get stuck in local minima.





==========================
Review F:
==========================

- First, who are the intended users of the proposed tool? Are they end users
  who might not have technical knowledge about data transformation programs? Or
  are they technical users who are unfamiliar with lens programs? I initially
  thought the tool is intended for the first type of users, but as I read
  through the paper it seems apparent that the authors intend to target the
  second group, especially with the requirement that users need to provide a
  regular expression description of the input/output data format. The
  requirement to do so seems to be the weakest point of the paper, especially
  given that existing tools (such as FlashFill or Dataplay [1]) do not have such
  requirement. If the tool is indeed intended for users who are not familiar
  with lens programming, then can the authors justify why lens is the
  appropriate programming model for data transformation? Wouldn't it be easier
  to dump the input/output data into a common format (say CSV, spreadsheet, or
  JSON), and simply use FlashFill or TriFacta [2] to learn the desired
  transformation program rather than providing the ? I understand that it might
  be difficult for tools like FlashFill to learn nested constructs (given its
  relational data model), but to me that doesn't quite justify the need for
  providing regular expressions as part of the input.

Discussed above


- Second, the evaluations seem weak.  The examples from FlashFill and are those
  that can already be synthesized using FlashFill by just providing input /
  output examples. Also, can the authors describe how complex are the regular
  expressions that users need to provide? Given that the same data format can be
  described using different semantically equivalent expressions, how do they
  affect the effectiveness of the synthesis process?

Discussed in part above.  Semantically equivalent expressions can have an impact
on the effectiveness of the synthesis process.  As we discuss in Speed of
Synthesis, in the Evaluation section, the difference in the presentation of the
regular expressions being mapped between is a stronger indicator of complexity
for this system.


- Finally, the real world contains many erroneous / dirty data. What happens if
  the user provide the incorrect regular expression or inconsistent input /
  output examples?

If there is possibly dirty data, then we expect this dirty data to not be
input to the transformer through the typing.  This would allow for the
unexpected data to fail, instead of silently succeed with unexpected results.
If this odd data is found to be an expected occurence, changes to the typing can
be made to address it, and a new lens can be synthesized.


- Can the authors describe what kind of examples were needed to
  achieve the efficient synthesis results? In light of the "Importance of
  Examples" subsection on p.11, it seems that choosing "clever" examples is one
  of the key requirement for the tool to succeed in synthesizing a valid
  program. In addition, is there some way for the tool to identify the error and
  perform iterative synthesis? Also, what happens if multiple lens program are
  found for a given input?

Clever examples do not need to be provided to the system.  The "Avg # Exmpl"
column gets its value from randomly generated examples, as we did not want our
implicit understanding of the synthesis algorithm to make our system look like
it needed fewer examples than it does when used by a non-expert.  When
multiple programs are possible, the one that requires the least significant
permutation is chosen.  The "Max # Exmpl" column corresponds to a permutation
generated which chooses a wrong permutation, if such a permutation is
permutation is possible.
