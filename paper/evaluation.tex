\section{Evaluation}
We ran our algorithm on tests validating our ability to handle regular
expression equivalences, tests derived from the Augeus test suite, and from
tests derived from the FlashFill tests.
The values presented are the arithmetic means of 10 test runs.
The tests were run on a 2.5 GHz Intel Core i7 processor, with
16 GB of 1600 MHz DDR3, running Mac OS X Yosemite.

\begin{figure*}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\bfseries Test & \bfseries ForceExpandTime & \bfseries ForceExpandExamplesRequired
& \bfseries Computation Time & \bfseries Examples Required
\csvreader[head to column names]{generated-data/data.csv}{}
{\\\hline\Test & \ForceExpandTime & \ForceExpandExamplesRequired & \ComputationTime & \ExamplesRequired}
\\\hline
\end{tabular}

\end{figure*}

\subsection{Speed}
Figure~\ref{fig:runtimes-table} provides the runtime of our algorithm on our
test suite.
In this figure, we want to minimize runtime, as a smaller runtime means we were
able to synthesize very efficiently.
We found that, on many of our difficult examples, our algorithm was able to find
the appropriate lens very quickly.

However, we can also see where the strengths and weaknesses are in our approach.
In particular, our synthesis strategy does quite well on problems that only
include a small number of transformations, even if the number of clauses is
decently large.
This is as expected, for after the appropriate transformations have been found,
the problem of finding the lens between the resulting DNF Regular Expressions
is merely as difficult as ordering the clauses.

For example, the address problem is the second slowest lens to synthesize.
This is because there is a large number of required expansions of user defined
data types, and there are multiple required star transformations.
Furthermore, there are a large number of potential star transformations because
of the large number of stars.
In addition to requiring a relatively deep search, this search is made much
more difficult because of the breadth of the search.
This shows us where our algorithm can hit issues.
When there are a large number of potential transformations, and there are a
large number of required transformations, our search gets more complicated,
and things slow down.

Another place where our synthesis algorithm preforms badly is where there is not
only a large number of clauses generated, but they also have a non-identity
lens between them.
For example, the double capitalization problem is another slowest test, despite
being an intuitively easy function to figure out, looking at it as a human.
This is because the algorithm problem has an exponential blow up in terms
of the number of clauses, when there are many regular expressions +d together.
If we take this problem to the logical extremes, we see that the the triple
capitalization problem takes 6.974 seconds, and the quadruple capitalization
problem takes 33 minutes and 54.730 seconds.

\subsection{Importance of Examples}
We wanted to see how many examples were required to synthesize the correct
bijective lens.
However, because of our implicit understanding of the algorithm, we fear we may
choose examples that we know will work well with the synthesis algorithm,
instead of choosing examples that users are likely to.
Because of that, we test the number of examples that it takes to generate the
correct lens, with randomly generated examples.


As we see in Figure~\ref{fig:examples-required-table}, examples are not too
important for synthesizing bijective lenses.\bcp{If that's really the
  conclusion, then maybe we should have written a paper about synthesizing
  bijective lenses {\em without\/} examples!  But I don't think we can
  actually conclude this from the examples we have.  }
Because bijective lenses have very few well typed programs, compared to the
number of functions, oftentimes just a few examples is sufficient.
As would be expected, the functions that have to make choices about the
locations of where data goes are those that require more examples.
In particular, the address problem requires more examples, because it must make
choices of where information goes, like which name is the a person's first name,
and which is their last.

\subsection{Importance of User Defined Data Types}

User defined data types are very critical to the algorithm and performance of
the synthesis algorithm.  User defined data types help direct the search of
which expansions should be performed.  Furthermore, they help to reduce the
number of clauses we have to deal with.  There is a potentially exponentially
large blowup in converting regular expressions into DNF form, but keeping
certain portions of the regular expressions abstracted lessens that blowup,
as those regular expressions are kept atomic in converting into DNF form.

User defined data types are also critical in the applications of previously
defined lenses.  We restrict the application of previously defined lenses to
only on user defined data types in the synthesis algorithm.  We then group the
user defined data types based on which have previously defined lenses between
them.  This allows for efficient recognition of when two user defined regular
expressions can be mapped to each other.  Removing the ability to use previously
defined lenses makes certain problems intractable, like (example1, example2,
example3).

While we can simulate the use of user defined data types automatically, by
giving regular expressions which have the same syntax the same user defined data
type, manually inputting them performs better in practice.  This is because the
procedure for automatically creating user defined data types creates far more
data types, which then creates a more complicated search problem.

%%% Local Variables:
%%% TeX-master: "main"
%%% End: