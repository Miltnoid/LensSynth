TARGET AUDIENCE

Several reviewers asked who the intended users of this tool are and
when the use case was compelling.  Briefly, the technology is aimed at
programmers.  If one were to ask random programmers whether they were
familiar with and knew how to use regular expressions, most would say
yes.  If one were to random programmers whether they were familiar
with lenses 100% would say no.  The central idea is to exploit
something that most programmers know well (regular expressions) to
help them write significant programs in a language that they do not
know well (lenses).  Programmers are often reluctant to learn new
things.  Our system helps them avoid that.

Several reviewers wondered whether regular expressions were really
necessary at all and whether we could get away without requiring them.
One of the reasons we feel our work is valuable is that it illustrates
an important practical tradeoff: by providing data formats, we can
infer transformations over significantly more sophisticated string
data types than related systems which do not use such information.
For instance, FlashFill cannot infer nested iterations.  17 of our
benchmarks have nested iterations and the deepest nesting structure
involved 4 levels of nesting.  As another example, FlashExtract does
not have unions, but it can represent them indirectly by using
options.  Still, in our tests (eg: on the Bibtex data source),
FlashExtract could not infer transformations between string data types
that contain unions.  By the way, both FlashExtract and FlashFill are
wonderful systems, but they are optimized for other problems.  Our
work demonstrates the tradeoff between designs.  We hope to make this
point more clearly in a revised version of the paper and to quantify
the number of benchmarks that we handle that cannot be handled by
other string synthesis systems.

Several reviewers wondered whether lenses really were tricky to write
and suggested we show a concrete example (thank you -- we should
have).  The following lens translates between idealized BibTeX and
EndNote author formats:

  let bibtex_to_endnote_au
    : (lens in BIBTEXAUTHORINFO <=> TAGGEDAUTHORDEFNS) =
    del "author={"
      . ins "au - "
      . lens_swap (NAME . del ",") (lens_swap WSP NAME)* 
      . (del " and "
           . ins "\n au - "
           . lens_swap (NAME . del ",") (lens_swap WSP NAME)* )* 
      . del "}" 

Writing this lens from scratch is quite tricky and fiddly.  The
trickiest part is the nested swaps, which are fiddly to think about
and get right.  Importantly, this is an incredibly simple example --
Augeas and Boomerang users regularly work with lenses significantly
larger than this.  For example, Boomerang's full version of an
idealized BibTeX converter is over 300 lines of code, and all but 4 of
the studied Augeas programs were over 50 lines of code, with 4 over
100 lines of code.

FORMALIZATION

A second major concern was the lack of a formal treatment of our
implementation, in particular the function RigidSynth.

In earlier drafts, we actually had a formal description of this
algorithm, but we (regrettably) removed it to get under the page limit
and inadvertently removed it from the extended version at the same
time.  We will reinstate the full treatment in the extended report and
find a good way to summarize it in the final paper.

The formal description is not very complex, but takes a good amount of
space to complete.  The formal description of RigidSynth first
describes an ordering on atoms, sequences, and DNF regular
expressions.  Next, this ordering is used to define a normalization
procedure for DNF regular expressions.  Finally, a procedure for
"zipping" two DNF regular expressions together is provided.  If these
two DNF regular expressions can be zipped, then there is a lens
between them.  A transformation from zipped DNF regular expressions
and the permutations that normalize them is then provided. These
formalizations are not very complex, but they take up a lot of space.

EVALUATION

A third area of concern was our evaluation, specifically the lack of
comparisons with other tools or a quantitative evaluation of the heuristics
we chose in searching for lenses.

Direct comparison with related tools is difficult because there aren't
any tools that do the same task: this is the first attempt at
bidirectional program synthesis.  Moreover, as mentioned above,
similar tools for unidirectional string transformation such as
FlashFill and FlashExtract can't easily be compared against our
benchmarks because they cannot handle the same kinds of
transformations.  We now recognize that a quantification of the
increase in expressibility would be important for readers of our
paper.  We will provide an analysis detailing how many of our programs
are inexpressible using other string synthesis tools in future
versions of our work.

Furthermore, we strongly agree that performance measurements quantifying the
importance of these aspects of the various heuristics used by the algorithm
would greatly improve the paper.  It will not be hard to instrument the
implementation so that we can disable heuristics individually and gather
these numbers.  (Anecdotally, we invented each of these heuristics when more
naive approaches failed.  For example, our initial search procedure, which
traversed arbitrary equivalences, could not synthesize anything beyond
simple microbenchmarks, even with the significant simplification of not
searching for potential compositions!  Without a priority queue, we were
able to synthesize complicated lenses when the source and target formats
were similar, but not when the formats differed significantly.)

BIJECTIVE LENS RESTRICTION

Our program is unable to synthesize general Boomerang lenses, but rather only
those which express bijections.
There certainly are limitations to restricting ourselves to bijective lenses.
Indeed, while all existing Boomerang and Augeas lenses can be transformed into
bijective lenses, few are bijective by default, often because of a lack of
bijectivity guarantees on whitespace.
We chose to restrict our domain to bijective programs as a simplification
technique for this new research, but think there are many directions this work
can be expanded to, and we feel this research is amenable to be expanded.

In particular, bijective quotient lenses, lenses which are bijective modulo an
equivalence on the source and target languages, are an expansion to this work
that already has shown promise.  Merely adding quotienting makes this tool more
practical.  XSugar - an existing, powerful language for XML and string
bidirectional transformations - expresses bijections modulo equivalences.
While we have only seen preliminary results so far, it seems that only minor
modifications were needed for our tool to generalize to bijective quotient
lenses.


We will clarify all these points in the next rewrite.

===========================================================================
===========================================================================
The remainder of this document -- to be read only as needed -- responds in
detail to individual concerns not already addressed.

==========================
Review A:
==========================

- The synthesized lenses are not shown or discussed in much detail,
  nor are they compared to hand-written ones (when possible).

Our implementation puts some work into making the synthesized lenses
readable.  The transformation from DNF lenses to Boomerang lenses alone
creates fairly unintelligible syntax.  This is an issue, we would like for
programmers to be able to validate the synthesized programs through a manual
inspection of them.  As such we apply a number of optimizations, including,
but not limited to:

1. Trivial sublenses are removed
2. Sublenses expressing identity transformations are turned into identities
3. Identity transformations are combined when possible
4. Distributed transformations are factored when possible

The previously shown lens, bibtex_to_endnote_au, is an autogenerated lens.
The primary difference we see between the autogenerated code, and what we would
write ourselves, is the repeated code of

ins "au - "
. lens_swap (NAME . del ",") (lens_swap WSP NAME)*

We personally do not find this to make the code significantly more difficult to
read.  However, adding this optimization is merely a rote application of common
subexpression elimination.


- The prevalence and limitations of bijective lenses (compared to
  less well-behaved ones) is not discussed.

There certainly are limitations to restricting ourselves to bijective lenses.
However, we chose the simplification of bijective lenses for this new research
area, but think there is a wealth of future work that can be done through an
investigation of less well-behaved lenses.

- How many of surveyed FlashFill examples were close to bijective? Were there
  only the 3 included in the evaluation?

Few of the surveyed FlashFill examples were close to bijective.  The tricks we
used to make our chosen 3 examples bijective - adding in extra fields to hold
bits of data - could be extended to non-bijective examples.  Our primary goal
with the use of FlashFill was to show that, when made bijective and given data
format descriptors, the problems FlashFill addresses become simple.


==========================
Review B:
==========================

- paper unclear in various (important) places

We unfortunately were had to be brief in our description of confluence,
bisimilarity, and definitional equivalence.  We go into much greater detail in
the appendix, and will make sure to refer to the appendix for a more detailed
description of these terms, and for a more formal understanding of their
importance.


- I am not sure the remark about the implementation spanning '5515 lines' is as
positive as you make it sound at the beginning of Section 6. There is no
pseudocode for the main part of the algorithm, a function called RigidSynth.
This makes one wonder how complex it really is and, more worryingly, as a
result, there is also no correctness proof and no complexity analysis.

Discussed in the part in the general section.  Many of the lines go not to the
core synthesis algorithm, but for language transformations, pretty printing,
lens simplification, and DFA operations.  The portion of our code corresponding
to RigidSynth is just over 400 lines.


- some proofs are missing

Please refer to our appendix, where we go into a detailed proof of these
theorems.





==========================
Review C:
==========================

- The discussion of related work can be improved. It is true that there aren't
  other approaches that are solving the same exact problem. However, some
  aspects of your synthesis are shared with other approaches.

Agreed, while we use similar techniques, they are used in different ways,
identifying the similarities and noting the differences would strengthen our
related work.





==========================
Review D:
==========================

- The algorithmic contributions (particularly, type-directedness) are often
  unclear.

Discussed in part in the general section.

RigidSynth is entirely type directed.  We use an ordering on the subcomponents
of the types to find which subcomponents must be mapped to each other.  After
these lenses are found, they are then combined into the final lens in a way
that keeps the lens well typed.

- Converting a regular expression to DNF may incur an exponential explosion (as
it is akin to determinizing an NFM). Have you encountered examples where that is
the case? Please state this shortcoming.

We have not encountered any examples in practice where this is a problem.
However, we include in our benchmarks cust/cap-prob and cust/2-cap-prob to
highlight this shortcoming.  Furthermore, we discuss the shortcoming in Speed of
Synthesis in the evaluation section.


- Have you considered using a SAT/SMT solver to search the space of
  permutations?

Because of our use of orderings, we are able to find the permutations in
O(n*log(n)) time.


- The experimental evaluation is limited and does not back the claimed
  contributions.  Similar to (a), what is the effect of type-directedness on
  the experimental results?

Discussed in part in the general section. We did not ever try an approach that
was not type-directed, as we do not think enumerative search followed by a
pruning of ill typed terms would scale well, as there are many ill typed terms.


- The fact that the avg # of examples is often 0 seems to indicate that the
  benchmarks are simple. This is perhaps because the language is very
  restricted; i.e., there are no user-defined lenses, only the core ones. Could
  you comment on that?

We discuss why the average number of examples is so low in Importance of
Examples in the evaluation section.  To highlight how complicated the input
specifications can be, the specification for the BibTeX to
EndNote conversion problem - one of our 10 least complex input formats - and the
specification for cron - our most complex format - are given
below.  We are happy to provide all our specifications.

cust/bib_prob
      #use "base.decls"
      typedef NAME = UPPERCASE (LOWERCASE)*;;
      
      typedef LASTCOMMASTART = NAME "," (WSP NAME)*;;
      
      typedef STARTTOEND = (NAME WSP)* NAME;;
      
      typedef BIBTEXAUTHORLIST = LASTCOMMASTART (" and " LASTCOMMASTART)*;;
      typedef BIBTEXAUTHORINFO = "author={" BIBTEXAUTHORLIST "}";;
      
      typedef AUTAG = "au - ";;
      typedef TITAG = "ti - ";;
      typedef JOTAG = "jo - ";;
      
      typedef TAGGEDAUTHORDEFNS = AUTAG STARTTOEND ("\n " AUTAG STARTTOEND)*;;
      
      typedef TITLE = NAME (WSP NAME)*;;
      typedef BIBTEXTITLE = "title={" TITLE "}";;
      typedef TAGGEDTITLE = TITAG TITLE;;
      
      typedef JOURNAL = NAME (WSP NAME)*;;
      typedef BIBTEXJOURNAL = "journal={" JOURNAL "}";;
      typedef TAGGEDJOURNAL = JOTAG JOURNAL;;
      
      typedef FULLBIBTEX = "{" ((BIBTEXJOURNAL | BIBTEXAUTHORINFO | BIBTEXTITLE)",")* "}";;
      typedef FULLTAGS = . | ((TAGGEDAUTHORDEFNS | TAGGEDTITLE | TAGGEDJOURNAL)
      (("\n" (TAGGEDAUTHORDEFNS | TAGGEDTITLE | TAGGEDJOURNAL))*)) ;;
      
      bibtex_to_readable_au = [BIBTEXAUTHORINFO <=> TAGGEDAUTHORDEFNS
      {"author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron}" <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon"}]
      
      bibtext_to_readable_title = [BIBTEXTITLE <=> TAGGEDTITLE
      {"title={Boomerang Resourceful Lenses For String Data}" <->
       "ti - Boomerang Resourceful Lenses For String Data"}]
      
      journal_to_readable_journal = [BIBTEXJOURNAL <=> TAGGEDJOURNAL
      {"journal={Principals Of Programming Languages}" <->
       "jo - Principals Of Programming Languages"}]
      
      bibtext_to_tagged_tester= [FULLBIBTEX <=> FULLTAGS {
      "{author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron},title={Boomerang Resourceful Lenses For String Data},journal={Principals Of Programming Languages},}"
      <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon
      ti - Boomerang Resourceful Lenses For String Data
      jo - Principals Of Programming Languages"
      }]
      
      test bibtext_to_tagged_tester
      {"{author={Foster, Nathan and Pierce, Benjamin and Bohannon, Aaron},}"
      <->
      "au - Nathan Foster
       au - Benjamin Pierce
       au - Aaron Bohannon",
      
      "{title={Boomerang Resourceful Lenses For String Data},}"
      <->
      "ti - Boomerang Resourceful Lenses For String Data",
      
      "{journal={Principals Of Programming Languages},}"
      <->
      "jo - Principals Of Programming Languages"
      };;

aug/cron
      #use "base.decls"
      #use "util.decls"
      
      typedef INDENT = (" " | "\t")*;;
      typedef INDENT_REQ = (" " | "\t")+;;
      typedef ALPHANUM = (UPPERCASE | LOWERCASE | DIGIT)+;;
      typedef RANGE = (ALPHANUM "-" ALPHANUM | ALPHANUM );;
      typedef PREFIX = "-";;
      
      typedef SCHEDULE_VALUE = "reboot" | "yearly" | "annually" | "monthly"
                             | "weekly" | "daily" | "midnight" | "hourly";;
      typedef SCHEDULE = "@" SCHEDULE_VALUE;;
      
      typedef USER = (UPPERCASE | LOWERCASE | DIGIT)+;;
      
      typedef TIME = NUMBER INDENT_REQ NUMBER INDENT_REQ NUMBER INDENT_REQ RANGE INDENT_REQ RANGE;;
      
      typedef SHELLCOMMAND_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|"  | "." ;;
      typedef SC_CHAR_OR_SPACE = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|" | "." | " " ;;
      typedef SHELLCOMMAND = (SHELLCOMMAND_CHAR (SC_CHAR_OR_SPACE)* SHELLCOMMAND_CHAR) | SHELLCOMMAND_CHAR;;
      
      typedef SHELLVAR_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_";; 
      typedef SHELLVAR_NAME = SHELLVAR_CHAR+;;
      typedef SHELLVALUE_CHAR = LOWERCASE | UPPERCASE | DIGIT | "_" | "/" | "|" | ".";;
      typedef SHELLVALUE_NAME = SHELLVALUE_CHAR+;;
      
      typedef SHELLVAR = SHELLVAR_NAME "=" SHELLVALUE_NAME "\n";;
      typedef COMMENTLINE = COMMENT "\n";;
      typedef ENTRY = INDENT (PREFIX | . ) (TIME | SCHEDULE) INDENT_REQ USER INDENT_REQ SHELLCOMMAND "\n";;
      typedef CRON = ( "\n" | SHELLVAR | COMMENTLINE | ENTRY)*;;
      
      typedef PREFIX_DICT = "{\"prefix\"=" ("true" | "false") "}";;
      typedef TIME_DICT = "{\"minute\"=" NUMBER ",\"ws1\"=" INDENT_REQ ",\"hour\"=" NUMBER 
        ",\"ws2\"=" INDENT_REQ ",\"dayofmonth\"=" NUMBER ",\"ws3\"=" INDENT_REQ 
        ",\"month\"=" RANGE ",\"ws4\"=" INDENT_REQ ",\"dayofweek\"=" RANGE "}";;
      typedef SCHEDULE_DICT = "{\"schedule\"=\"" SCHEDULE_VALUE "\"}";;
      typedef ENTRY_DICT = "{\"indent\"=\"" INDENT "\"," PREFIX_DICT "," (TIME_DICT | SCHEDULE_DICT)
        ",\"indent2\"=\"" INDENT_REQ "\",\"user\"=\"" USER "\",\"indent3\"=\""
        INDENT_REQ "\",\"command\"=\"" SHELLCOMMAND "\"}";;
      typedef SHELL_DICT = "{\"varname\"=\"" SHELLVAR_NAME "\",\"value\"=\"" SHELLVALUE_NAME "\"}";;
      typedef CRON_DICT = ((EMPTYDICT | SHELL_DICT | COMMENT_DICT | ENTRY_DICT) "\n")*;;
      
      cron_lens = [CRON_DICT <=> CRON {}]





==========================
Review E:
==========================

- For the BibTeX to EndNote transformation example, the paper presents an
  algorithmic way to align the regular expressions to find corresponding
  permutations of sub-expressions to match different fields. Why can’t the user
  provide regular expressions in first place that are aligned? For example,
  there is an epsilon in the BibTeX regular expression, but there is no epsilon
  in the EndNote format. Wouldn’t it be a more clear and a better specification
  on user’s part to write a regular expression where EndNode format also has an
  epsilon with an Or in front. For finding permutations as well, it isn’t clear
  why can’t the user simply annotate or label different fields using some user
  interface, rather than the algorithm having to search over different matching
  of sub-expressions.

We feel that requiring well aligned regular expressions is too large a burden on
the user.  The user themselves needs to think through various equivalence rules,
which we think is better suited to an automated tool.

This is very difficult when the equivalences that need to be exploited are
nonobvious or hidden away.  Consider the two regular expressions (A|B)C*
and (A|C(C*)A)|(B|B(C*)C).  It is nontrivial for a user to identify that the
equivalences that must be applied to reach two well aligned lenses.
Furthermore, the aligned regular expressions could become difficult to read and
maintain: (A|B)(e+C+CC*C) is less understandable than (A|B)C*.

The idea of adding in the ability to annotate fields is an interesting one.
The way that examples are embedded in the regular expressions is similar to an
annotation.  In certain scenarios, it may also be easier to give annotations
than to give a covering example.  Likely the best solution would be one which
uses both annotations and examples to help differentiate similar fields.


- Is there a restriction that the user-provided regular expressions need to
  parse the source and target formats unambiguously? For example, if a user
  provides the regular expression for a format as Name* Name*, and the input is
  John Conway, would the system disallow such user inputs?

The regular expressions provided to the synthesis algorithm must be unambiguous.
However, we provide machinery to allow ambiguous user-defined regular
expressions to safely be subexpressions of the input format descriptors.
Using the keyword "abstract" on the definition of a user-defined regular
expression allows these abstract regular expressions to never be expanded into
their subcomponents in the search procedure.  With this opacity, ambiguous
regular expressions can safely be subexpressions of an input format descriptor,
though only the identity transformation can act on them.

- The synthesis algorithm section presents several heuristics to assign scores
  and pseudometric scores to a pair of DNF regular expressions. How were these
  heuristics chosen? What is the effect of these heuristics on the benchmark
  problems?

Discussed in part in the general section. These heuristics were chosen in a way
to leverage the abstractions that programmers use for their specifications,
while also allowing enough breadth to the search space to not get stuck in
local minima.





==========================
Review F:
==========================

- Second, the evaluations seem weak.  The examples from FlashFill and are those
  that can already be synthesized using FlashFill by just providing input /
  output examples. Also, can the authors describe how complex are the regular
  expressions that users need to provide? Given that the same data format can be
  described using different semantically equivalent expressions, how do they
  affect the effectiveness of the synthesis process?

Discussed in part in the general section. Semantically equivalent expressions
can have an impact on the effectiveness of the synthesis process.  As we discuss
in Speed of Synthesis, in the Evaluation section, the difference in the
presentation of the regular expressions being mapped between is a stronger
indicator of complexity for this system.


- Finally, the real world contains many erroneous / dirty data. What happens if
  the user provide the incorrect regular expression or inconsistent input /
  output examples?

If there is possibly dirty data, then we expect this dirty data to not be
input to the transformer through the typing.  This would allow for the
unexpected data to fail, instead of silently succeed with unexpected results.
If this odd data is found to be an expected occurence, changes to the typing can
be made to address it, and a new lens can be synthesized.


- Can the authors describe what kind of examples were needed to
  achieve the efficient synthesis results? In light of the "Importance of
  Examples" subsection on p.11, it seems that choosing "clever" examples is one
  of the key requirement for the tool to succeed in synthesizing a valid
  program. In addition, is there some way for the tool to identify the error and
  perform iterative synthesis? Also, what happens if multiple lens program are
  found for a given input?

Clever examples do not need to be provided to the system.  The "Avg # Exmpl"
column gets its value from randomly generated examples, as we did not want our
implicit understanding of the synthesis algorithm to make our system look like
it needed fewer examples than it does when used by a non-expert.  When
multiple programs are possible, the one that requires the least significant
permutation is chosen.  The "Max # Exmpl" column corresponds to a permutation
generated which chooses a wrong permutation, if such a permutation is
permutation is possible.
